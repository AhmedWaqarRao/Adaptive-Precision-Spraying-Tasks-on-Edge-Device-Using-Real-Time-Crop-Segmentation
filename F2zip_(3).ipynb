{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANGHK8YUuMWb"
      },
      "source": [
        "# **Algorithm 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "4D9-eNZ4Nzmr",
        "outputId": "0cb58b50-a192-47a7-dfca-7220cb1ecaf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Scenario Complexity (Beta): 0.8662936091423035\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SSD300_VGG16_Weights.COCO_V1`. You can also use `weights=SSD300_VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Channel Importance Scores Computed\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAHHCAYAAACiOWx7AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAASnxJREFUeJzt3XlcFWX///H3AWQRWdwAuSX1NnNJ09I0XDNJXLIsl0xLXNIWcLfUyrWMtFvvNEur21u8U8usNLU0ca8kxX1fyj0FLZUjmKgwvz/8Mj+PoI6InIO+no/HeTw811xzzWfGA+fNzHXm2AzDMAQAAIDrcnN2AQAAAAUBoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJuEOVLVtWXbp0ue3bOXjwoGw2m+Li4sy2Ll26qEiRIrd921lsNptGjBiRb9vLMmLECNlstnzfLgDnIDTBpcTFxclms13z8euvv970mD/88INT3lDz0qOPPmoeAzc3N/n7+6tixYp64YUXFB8fn2fbceVj5cq15TW73a6RI0eqevXqKlKkiHx8fFS1alUNGjRIx44dM/vlFE6vfK1c/di9e7dD39q1a8tms2ny5Mk51nH1z6OHh4f+8Y9/qEuXLvrjjz8s7cvx48c1ePBgNW7cWH5+frLZbFq5cuU1+69Zs0b169dX4cKFFRISot69eys1NfWG28kK71mPQoUKqUSJEqpbt67eeOMNHT582FK9OTl27JhGjBihzZs353qMvHQ3/Sy4Gg9nFwDkZNSoUSpXrly29nvvvfemx/rhhx/00UcfFfhfMqVLl1ZsbKwkKS0tTb/99pu+/fZbzZgxQ+3bt9eMGTNUqFAhs/+ePXvk5nZzfxfl5liVKVNGf//9t8O2b4fr1fb333/LwyP/f5299dZbGjx4cJ6OuX//fkVEROjw4cNq166devbsKU9PT23dulVTp07V3LlztXfv3uuOceVr5UqhoaHmv/ft26fExESVLVtWM2fO1CuvvHLN8bJ+Hs+fP69ff/1VcXFx+vnnn7V9+3Z5e3tft5Y9e/ZozJgxqlChgqpVq6aEhIRr9t28ebOaNGmiypUra/z48Tp69Kj+9a9/ad++fVq0aNF1t5PlueeeU4sWLZSZmanTp08rMTFRH3zwgSZMmKCpU6eqQ4cOlsa50rFjxzRy5EiVLVtWNWrUuOn189qd8jutICI0wSU1b95ctWrVyvftXrp0SZmZmfL09Mz3bd9IQECAnn/+eYe29957T71799bHH3+ssmXLasyYMeYyLy+v21rPlcfqRm+ct5uztu/h4ZGnYe3SpUt65plnlJycrJUrV6p+/foOy0ePHu3wf3wtOb1WrjZjxgwFBQVp3Lhxatu2rQ4ePKiyZcvm2PfKn8cXX3xRJUqU0JgxYzR//ny1b9/+utupWbOm/vrrLxUrVkxff/212rVrd82+b7zxhooWLaqVK1fK399f0uXLzD169NCSJUvUtGnT625Lkh566KFs+37o0CE1bdpUUVFRqly5sqpXr37DcYCccHkOBVLWqfh//etf+vTTT1W+fHl5eXnp4YcfVmJiotmvS5cu+uijjyTJ4dT91WN88MEH5hg7d+6UJC1fvlwNGjSQr6+vAgMD9dRTT2nXrl0OdWTNadm9e7fat28vf39/FS9eXH369NH58+fNfo0aNbrmL+qKFSsqMjIyV8fB3d1dEydOVJUqVTRp0iSlpKSYy66e03Tx4kWNHDlSFSpUkLe3t4oXL6769eubl/dye6xymtOUZf/+/YqMjJSvr69CQ0M1atQoGYZhLl+5cmWOl2uuHvN6tWW1Xf1X96ZNm9S8eXP5+/urSJEiatKkSbbLu1mXn3755Rf1799fJUuWlK+vr55++mmdPHnyhsc/pzlNNptNMTExmjdvnqpWrSovLy/df//9Wrx48Q3H++abb7Rlyxa9+eab2QKTJPn7+2v06NE3HMeKWbNmqW3btnriiScUEBCgWbNmWV63QYMGkqTff//9hn39/PxUrFixG/az2+2Kj4/X888/bwYmSercubOKFCmir776ynJ9VytTpozi4uJ04cIFjR071mw/deqUBg4cqGrVqqlIkSLy9/dX8+bNtWXLFrPPypUr9fDDD0uSunbtar72sl6bP/30k9q1a6d77rlHXl5eCgsLU79+/fT333871JCUlKSuXbuqdOnS8vLyUqlSpfTUU0/p4MGDDv0WLVpk/t7x8/NTy5YttWPHDnP5jX4WcHtxpgkuKSUlRX/++adDm81mU/HixR3aZs2apbNnz+qll16SzWbT2LFj9cwzz2j//v0qVKiQXnrpJR07dkzx8fH6/PPPc9zWtGnTdP78efXs2VNeXl4qVqyYli5dqubNm+uf//ynRowYob///lsffvih6tWrp40bN2b7i7x9+/YqW7asYmNj9euvv2rixIk6ffq0/ve//0mSXnjhBfXo0UPbt29X1apVzfUSExO1d+9evfXWW7k+Vu7u7nruuec0dOhQ/fzzz2rZsmWO/UaMGKHY2Fi9+OKLql27tux2u9avX6+NGzfq8ccfz/WxyszMzLFvRkaGmjVrpkceeURjx47V4sWLNXz4cF26dEmjRo26qX20UtuVduzYoQYNGsjf31+vv/66ChUqpE8++USPPvqoVq1apTp16jj079Wrl4oWLarhw4fr4MGD+uCDDxQTE6PZs2ffVJ1Zfv75Z3377bd69dVX5efnp4kTJ6pNmzY6fPhwttfwlebPny/p8uvlVmRkZGT7+fH29jbnP61du1a//fabpk2bJk9PTz3zzDOaOXOm3njjDUvjZ73RFy1a9JbqvNK2bdt06dKlbGeYPT09VaNGDW3atOmWxg8PD1f58uUd5gDu379f8+bNU7t27VSuXDklJyfrk08+UaNGjbRz506FhoaqcuXKGjVqlIYNG6aePXuagbFu3bqSpDlz5ujcuXN65ZVXVLx4ca1bt04ffvihjh49qjlz5pjbatOmjXbs2KFevXqpbNmyOnHihOLj43X48GHz98nnn3+uqKgoRUZGasyYMTp37pwmT56s+vXra9OmTSpbtuxN/ywgjxmAC5k2bZohKceHl5eX2e/AgQOGJKN48eLGqVOnzPbvvvvOkGQsWLDAbIuOjjZyeqlnjeHv72+cOHHCYVmNGjWMoKAg46+//jLbtmzZYri5uRmdO3c224YPH25IMp588kmH9V999VVDkrFlyxbDMAzjzJkzhre3tzFo0CCHfr179zZ8fX2N1NTU6x6XRo0aGffff/81l8+dO9eQZEyYMMFsK1OmjBEVFWU+r169utGyZcvrbic3xypr2bRp08y2qKgoQ5LRq1cvsy0zM9No2bKl4enpaZw8edIwDMNYsWKFIclYsWLFDce8Vm2GYRiSjOHDh5vPW7dubXh6ehq///672Xbs2DHDz8/PaNiwodmW9XqLiIgwMjMzzfZ+/foZ7u7uxpkzZ3LcXpas//+ra/H09DR+++03s23Lli2GJOPDDz+87ngPPvigERAQcN0+V4qKijJ8fX0d2ho1apTjz8+Vr4WYmBgjLCzM3OclS5YYkoxNmzY5jJV1fJYuXWqcPHnSOHLkiPH1118bJUuWNLy8vIwjR45YrtUwDGPOnDk5/n9fuWz16tXZlrVr184ICQm57thZr5n333//mn2eeuopQ5KRkpJiGIZhnD9/3sjIyMg2jpeXlzFq1CizLTExMdvrMcu5c+eytcXGxho2m804dOiQYRiGcfr06RvWdvbsWSMwMNDo0aOHQ3tSUpIREBDg0H69nwXcXlyeg0v66KOPFB8f7/DIaSLos88+6/DXbtZfgfv377e8rTZt2qhkyZLm8+PHj2vz5s3q0qWLw2WFBx54QI8//rh++OGHbGNER0c7PO/Vq5ckmX0DAgL01FNP6YsvvjAvT2VkZGj27Nlq3bq1fH19Ldebk6wzCGfPnr1mn8DAQO3YsUP79u3L9XauPlY3EhMTY/4767LVhQsXtHTp0lzXcCMZGRlasmSJWrdurX/+859me6lSpdSxY0f9/PPPstvtDuv07NnT4RJHgwYNlJGRoUOHDuWqhoiICJUvX958/sADD8jf3/+Gr0u73S4/P79cbfNKZcuWzfbz8/rrr0u6PG9q9uzZevbZZ819fuyxxxQUFKSZM2dec39KliypsLAwtW3bVr6+vpo/f75Kly59y7VmybqcldNcPG9v72yXu3Lj6p8TLy8v88MSGRkZ+uuvv1SkSBFVrFhRGzdutDSmj4+P+e+0tDT9+eefqlu3rgzDMM+O+fj4yNPTUytXrtTp06dzHCc+Pl5nzpzRc889pz///NN8uLu7q06dOlqxYkWu9xt5h8tzcEm1a9e2NBH8nnvucXieFaCu9YspJ1d/Si/rjbJixYrZ+lauXFk//vij0tLSHIJOhQoVHPqVL19ebm5uDvMVOnfurNmzZ+unn35Sw4YNtXTpUiUnJ9/ypRhJ5keyr/eGO2rUKD311FO67777VLVqVTVr1kwvvPCCHnjgAcvbyekTjdfi5ubmEFok6b777pOkbPM48tLJkyd17ty5a/7/ZWZm6siRI7r//vvN9rx4HV3p6vGyxrzReFaClRW+vr6KiIjIcdmSJUt08uRJ1a5dW7/99pvZ3rhxY33xxRcaM2ZMtk9dfvTRR7rvvvuUkpKi//73v1q9erVDuLlw4YJOnTrlsE7JkiXl7u5uueas8JGenp5t2fnz5x3CSW5d/XOSmZmpCRMm6OOPP9aBAweUkZFh9r3eZdQrHT58WMOGDdP8+fOz/f9mzTH08vLSmDFjNGDAAAUHB+uRRx7RE088oc6dOyskJESSzD9mHnvssRy3c+U8LzgPoQkF2rV+KRtXTDa+kbz4ZXy1nCZmRkZGKjg4WDNmzFDDhg01Y8YMhYSEXPPN7WZs375d0vVvydCwYUP9/vvv+u6777RkyRL95z//0b///W9NmTJFL774oqXt5PWxutYE1ivfvPJDXryO8mK8SpUqadOmTTpy5IjCwsJyte0byTqbdK1Pva1atUqNGzd2aLvyj5jWrVurfv366tixo/bs2aMiRYpozZo12dY5cODANT+Nl5NSpUpJunym92rHjx93uF1Cbm3fvl1BQUFmAHn33Xc1dOhQdevWTW+//baKFSsmNzc39e3b95pz9a6UkZGhxx9/XKdOndKgQYNUqVIl+fr66o8//lCXLl0cxujbt69atWqlefPm6ccff9TQoUMVGxur5cuX68EHHzT7fv7552aQupIzbqmB7PhfwB3vZj9ZUqZMGUmX7y9ztd27d6tEiRLZLqft27fP4SzMb7/9pszMTIc3DXd3d3Xs2FFxcXEaM2aM5s2bpx49etzUX+M5ycjI0KxZs1S4cOEcP3F1pWLFiqlr167q2rWrUlNT1bBhQ40YMcIMTXn5KZzMzEzt37/fPLskyby/UNZxyTqjc+bMGYd1c7osZrW2kiVLqnDhwtf8/3Nzc7ttgeRWtWrVSl988YVmzJihIUOG5Pn4aWlp+u677/Tss8+qbdu22Zb37t1bM2fOzBaAruTu7q7Y2Fg1btxYkyZN0uDBg1W9evVsN1nN6Y3/eqpWrSoPDw+tX7/eIdBduHBBmzdvvuGtDW4kISFBv//+u8PtCL7++ms1btxYU6dOdeh75swZlShRwnx+rdfetm3btHfvXk2fPl2dO3c22691w9ny5ctrwIABGjBggPbt26caNWpo3LhxmjFjhnk5Nygo6IZ/SPFpOedhThPueFkB5+o35mspVaqUatSooenTpzuss337di1ZskQtWrTItk7WR4CzfPjhh5Iu39/mSi+88IJOnz6tl156SampqTe8l86NZGRkqHfv3tq1a5d69+593VP4f/31l8PzIkWK6N5773W4HHKzx+pGJk2aZP7bMAxNmjRJhQoVUpMmTSRdDqju7u5avXq1w3off/xxtrGs1ubu7q6mTZvqu+++c7gMmJycrFmzZql+/foue6mjbdu2qlatmkaPHp3jTSDPnj2rN998M9fjz507V2lpaYqOjlbbtm2zPZ544gl98803OV4iu9Kjjz6q2rVr64MPPtD58+dVtGhRRUREODxu9t5ZAQEBioiI0IwZMxzm5n3++edKTU297v2dbuTQoUPq0qWLPD099dprr5nt7u7u2c7+zZkzJ9vdzq/12sv6g+fKMQzD0IQJExz6nTt3zuEWJNLlAOXn52ce68jISPn7++vdd9/VxYsXs+3DlbfAyOufU1jHmSa4pEWLFmX7ygfp8sd8r54ncyM1a9aUdPmv6MjISLm7u9/wrsDvv/++mjdvrvDwcHXv3t285UBAQECOd+E9cOCAnnzySTVr1kwJCQmaMWOGOnbsmO3eTA8++KCqVq2qOXPmqHLlynrooYcs70dKSopmzJgh6fIv4aw7gv/+++/q0KGD3n777euuX6VKFT366KOqWbOmihUrpvXr1+vrr792mKydm2N1Ld7e3lq8eLGioqJUp04dLVq0SN9//73eeOMNczJ5QECA2rVrpw8//FA2m03ly5fXwoULdeLEiWzj3Uxt77zzjuLj41W/fn29+uqr8vDw0CeffKL09HSH+/S4mkKFCunbb79VRESEGjZsqPbt26tevXoqVKiQduzYoVmzZqlo0aK5vlfTzJkzVbx4cfPj8ld78skn9dlnn+n777/XM888c92xXnvtNbVr105xcXF6+eWXr9v3nXfekSTzfkOff/65fv75Z0lyuN3G6NGjVbduXTVq1Eg9e/bU0aNHNW7cODVt2lTNmjWztI8bN27UjBkzlJmZqTNnzigxMVHffPONbDabPv/8c4c5fE888YRGjRqlrl27qm7dutq2bZtmzpyZ7XdM+fLlFRgYqClTpsjPz0++vr6qU6eOKlWqpPLly2vgwIH6448/5O/vr2+++Sbb3Ka9e/eqSZMmat++vapUqSIPDw/NnTtXycnJ5mvY399fkydP1gsvvKCHHnpIHTp0UMmSJXX48GF9//33qlevnvlHSF7+nOImOe1ze0AOrnfLAV3xkd/rfbxYV338/NKlS0avXr2MkiVLGjabzfyo7o0+orx06VKjXr16ho+Pj+Hv72+0atXK2Llzp0OfrI+c79y502jbtq3h5+dnFC1a1IiJiTH+/vvvHMcdO3asIcl49913LR+Xqz9GXqRIEaNChQrG888/byxZsiTHda6+5cA777xj1K5d2wgMDDR8fHyMSpUqGaNHjzYuXLhwS8fqWrcc8PX1NX7//XejadOmRuHChY3g4GBj+PDh2T7iffLkSaNNmzZG4cKFjaJFixovvfSSsX379mxjXqs2w8j+f24YhrFx40YjMjLSKFKkiFG4cGGjcePGxpo1axz6ZL3eEhMTHdqvdSuEq13rlgPR0dHZ+l79/3E9p0+fNoYNG2ZUq1bNKFy4sOHt7W1UrVrVGDJkiHH8+HGz37VuOZDT7SmSk5MNDw8P44UXXrjmds+dO2cULlzYePrppw3DuPbxMQzDyMjIMMqXL2+UL1/euHTp0nX353o/01f76aefjLp16xre3t5GyZIljejoaMNut193fMP4/6/DrIeHh4dRrFgxo06dOsaQIUPMj/9f6fz588aAAQOMUqVKGT4+Pka9evWMhIQEo1GjRkajRo0c+n733XdGlSpVDA8PD4fX5s6dO42IiAijSJEiRokSJYwePXqYt5jI6vPnn38a0dHRRqVKlQxfX18jICDAqFOnjvHVV19lq2nFihVGZGSkERAQYHh7exvly5c3unTpYqxfv97sc72fBdxeNsPI5UxHABoxYoRGjhypkydPOsyBuJ4JEyaoX79+OnjwYI6fsgIAuCbmNAH5yDAMTZ06VY0aNSIwAUABw5wmIB+kpaVp/vz5WrFihbZt26bvvvvO2SUBAG4SoQnIBydPnlTHjh0VGBioN954Q08++aSzSwIA3CTmNAEAAFjAnCYAAAALCE0AAAAWMKcpj2RmZurYsWPy8/PjFvcAABQQhmHo7NmzCg0NzfZl1VcjNOWRY8eOuez3WQEAgOs7cuSISpcufd0+hKY84ufnJ+nyQXfV77UCAACO7Ha7wsLCzPfx6yE05ZGsS3L+/v6EJgAAChgrU2uYCA4AAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAAC5wamlavXq1WrVopNDRUNptN8+bNM5ddvHhRgwYNUrVq1eTr66vQ0FB17txZx44dcxjj1KlT6tSpk/z9/RUYGKju3bsrNTXVoc/WrVvVoEEDeXt7KywsTGPHjs1Wy5w5c1SpUiV5e3urWrVq+uGHH27LPgMAgILJqaEpLS1N1atX10cffZRt2blz57Rx40YNHTpUGzdu1Lfffqs9e/boySefdOjXqVMn7dixQ/Hx8Vq4cKFWr16tnj17msvtdruaNm2qMmXKaMOGDXr//fc1YsQIffrpp2afNWvW6LnnnlP37t21adMmtW7dWq1bt9b27dtv384DAIACxWYYhuHsIiTJZrNp7ty5at269TX7JCYmqnbt2jp06JDuuece7dq1S1WqVFFiYqJq1aolSVq8eLFatGiho0ePKjQ0VJMnT9abb76ppKQkeXp6SpIGDx6sefPmaffu3ZKkZ599VmlpaVq4cKG5rUceeUQ1atTQlClTLNVvt9sVEBCglJQU+fv75/IoAACA/HQz798e+VRTnkhJSZHNZlNgYKAkKSEhQYGBgWZgkqSIiAi5ublp7dq1evrpp5WQkKCGDRuagUmSIiMjNWbMGJ0+fVpFixZVQkKC+vfv77CtyMhIh8uFV0tPT1d6err53G63581OAgCQD1q1yv26CxbkXR0FSYGZCH7+/HkNGjRIzz33nJkEk5KSFBQU5NDPw8NDxYoVU1JSktknODjYoU/W8xv1yVqek9jYWAUEBJiPsLCwW9tBAADg0gpEaLp48aLat28vwzA0efJkZ5cjSRoyZIhSUlLMx5EjR5xdEgAAuI1c/vJcVmA6dOiQli9f7nC9MSQkRCdOnHDof+nSJZ06dUohISFmn+TkZIc+Wc9v1CdreU68vLzk5eWV+x0DAAAFikufacoKTPv27dPSpUtVvHhxh+Xh4eE6c+aMNmzYYLYtX75cmZmZqlOnjtln9erVunjxotknPj5eFStWVNGiRc0+y5Ytcxg7Pj5e4eHht2vXAABAAePU0JSamqrNmzdr8+bNkqQDBw5o8+bNOnz4sC5evKi2bdtq/fr1mjlzpjIyMpSUlKSkpCRduHBBklS5cmU1a9ZMPXr00Lp16/TLL78oJiZGHTp0UGhoqCSpY8eO8vT0VPfu3bVjxw7Nnj1bEyZMcJj43adPHy1evFjjxo3T7t27NWLECK1fv14xMTH5fkwAAIBrcuotB1auXKnGjRtna4+KitKIESNUrly5HNdbsWKFHn30UUmXb24ZExOjBQsWyM3NTW3atNHEiRNVpEgRs//WrVsVHR2txMRElShRQr169dKgQYMcxpwzZ47eeustHTx4UBUqVNDYsWPVokULy/vCLQcAAAUJn5677Gbev13mPk0FHaEJAFCQEJouu5n3b5ee0wQAAOAqCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACp4am1atXq1WrVgoNDZXNZtO8efMclhuGoWHDhqlUqVLy8fFRRESE9u3b59Dn1KlT6tSpk/z9/RUYGKju3bsrNTXVoc/WrVvVoEEDeXt7KywsTGPHjs1Wy5w5c1SpUiV5e3urWrVq+uGHH/J8fwEAQMHl1NCUlpam6tWr66OPPspx+dixYzVx4kRNmTJFa9eula+vryIjI3X+/HmzT6dOnbRjxw7Fx8dr4cKFWr16tXr27Gkut9vtatq0qcqUKaMNGzbo/fff14gRI/Tpp5+afdasWaPnnntO3bt316ZNm9S6dWu1bt1a27dvv307DwAAChSbYRiGs4uQJJvNprlz56p169aSLp9lCg0N1YABAzRw4EBJUkpKioKDgxUXF6cOHTpo165dqlKlihITE1WrVi1J0uLFi9WiRQsdPXpUoaGhmjx5st58800lJSXJ09NTkjR48GDNmzdPu3fvliQ9++yzSktL08KFC816HnnkEdWoUUNTpkyxVL/dbldAQIBSUlLk7++fV4cFAIDbolWr3K+7YEHe1eFsN/P+7bJzmg4cOKCkpCRFRESYbQEBAapTp44SEhIkSQkJCQoMDDQDkyRFRETIzc1Na9euNfs0bNjQDEySFBkZqT179uj06dNmnyu3k9UnazsAAAAezi7gWpKSkiRJwcHBDu3BwcHmsqSkJAUFBTks9/DwULFixRz6lCtXLtsYWcuKFi2qpKSk624nJ+np6UpPTzef2+32m9k9AABQwLjsmSZXFxsbq4CAAPMRFhbm7JIAAMBt5LJnmkJCQiRJycnJKlWqlNmenJysGjVqmH1OnDjhsN6lS5d06tQpc/2QkBAlJyc79Ml6fqM+WctzMmTIEPXv3998brfbCU4AgLvC3TofymXPNJUrV04hISFatmyZ2Wa327V27VqFh4dLksLDw3XmzBlt2LDB7LN8+XJlZmaqTp06Zp/Vq1fr4sWLZp/4+HhVrFhRRYsWNftcuZ2sPlnbyYmXl5f8/f0dHgAA4M7l1NCUmpqqzZs3a/PmzZIuT/7evHmzDh8+LJvNpr59++qdd97R/PnztW3bNnXu3FmhoaHmJ+wqV66sZs2aqUePHlq3bp1++eUXxcTEqEOHDgoNDZUkdezYUZ6enurevbt27Nih2bNna8KECQ5nifr06aPFixdr3Lhx2r17t0aMGKH169crJiYmvw8JAABwUU69PLd+/Xo1btzYfJ4VZKKiohQXF6fXX39daWlp6tmzp86cOaP69etr8eLF8vb2NteZOXOmYmJi1KRJE7m5ualNmzaaOHGiuTwgIEBLlixRdHS0atasqRIlSmjYsGEO93KqW7euZs2apbfeektvvPGGKlSooHnz5qlq1ar5cBQAAEBB4DL3aSrouE8TAKAguZV5SbfC1eY03RH3aQIAAHAlhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWODh7AIAAMDNa9XK2RXcfTjTBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMAClw5NGRkZGjp0qMqVKycfHx+VL19eb7/9tgzDMPsYhqFhw4apVKlS8vHxUUREhPbt2+cwzqlTp9SpUyf5+/srMDBQ3bt3V2pqqkOfrVu3qkGDBvL29lZYWJjGjh2bL/sIAAAKBpcOTWPGjNHkyZM1adIk7dq1S2PGjNHYsWP14Ycfmn3Gjh2riRMnasqUKVq7dq18fX0VGRmp8+fPm306deqkHTt2KD4+XgsXLtTq1avVs2dPc7ndblfTpk1VpkwZbdiwQe+//75GjBihTz/9NF/3FwAAuC6bceVpGxfzxBNPKDg4WFOnTjXb2rRpIx8fH82YMUOGYSg0NFQDBgzQwIEDJUkpKSkKDg5WXFycOnTooF27dqlKlSpKTExUrVq1JEmLFy9WixYtdPToUYWGhmry5Ml68803lZSUJE9PT0nS4MGDNW/ePO3evdtSrXa7XQEBAUpJSZG/v38eHwkAABy1auXsCnJnwQJnV+DoZt6/XfpMU926dbVs2TLt3btXkrRlyxb9/PPPat68uSTpwIEDSkpKUkREhLlOQECA6tSpo4SEBElSQkKCAgMDzcAkSREREXJzc9PatWvNPg0bNjQDkyRFRkZqz549On36dI61paeny263OzwAAMCdy8PZBVzP4MGDZbfbValSJbm7uysjI0OjR49Wp06dJElJSUmSpODgYIf1goODzWVJSUkKCgpyWO7h4aFixYo59ClXrly2MbKWFS1aNFttsbGxGjlyZB7sJQAAKAhc+kzTV199pZkzZ2rWrFnauHGjpk+frn/961+aPn26s0vTkCFDlJKSYj6OHDni7JIAAMBt5NJnml577TUNHjxYHTp0kCRVq1ZNhw4dUmxsrKKiohQSEiJJSk5OVqlSpcz1kpOTVaNGDUlSSEiITpw44TDupUuXdOrUKXP9kJAQJScnO/TJep7V52peXl7y8vK69Z0EAAAFgkufaTp37pzc3BxLdHd3V2ZmpiSpXLlyCgkJ0bJly8zldrtda9euVXh4uCQpPDxcZ86c0YYNG8w+y5cvV2ZmpurUqWP2Wb16tS5evGj2iY+PV8WKFXO8NAcAAO4+Lh2aWrVqpdGjR+v777/XwYMHNXfuXI0fP15PP/20JMlms6lv37565513NH/+fG3btk2dO3dWaGioWrduLUmqXLmymjVrph49emjdunX65ZdfFBMTow4dOig0NFSS1LFjR3l6eqp79+7asWOHZs+erQkTJqh///7O2nUAAOBiXPry3IcffqihQ4fq1Vdf1YkTJxQaGqqXXnpJw4YNM/u8/vrrSktLU8+ePXXmzBnVr19fixcvlre3t9ln5syZiomJUZMmTeTm5qY2bdpo4sSJ5vKAgAAtWbJE0dHRqlmzpkqUKKFhw4Y53MsJAADc3Vz6Pk0FCfdpAgDkJ+7TlDfumPs0AQAAuApCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMCCXIWm/fv353UdAAAALi1Xoenee+9V48aNNWPGDJ0/fz6vawIAAHA5uQpNGzdu1AMPPKD+/fsrJCREL730ktatW5fXtQEAALiMXIWmGjVqaMKECTp27Jj++9//6vjx46pfv76qVq2q8ePH6+TJk3ldJwAAgFPd0kRwDw8PPfPMM5ozZ47GjBmj3377TQMHDlRYWJg6d+6s48eP51WdAAAATnVLoWn9+vV69dVXVapUKY0fP14DBw7U77//rvj4eB07dkxPPfVUXtUJAADgVB65WWn8+PGaNm2a9uzZoxYtWuh///ufWrRoITe3yxmsXLlyiouLU9myZfOyVgAAAKfJVWiaPHmyunXrpi5duqhUqVI59gkKCtLUqVNvqTgAAABXkavQtG/fvhv28fT0VFRUVG6GBwAAcDm5mtM0bdo0zZkzJ1v7nDlzNH369FsuCgAAwNXkKjTFxsaqRIkS2dqDgoL07rvv3nJRAAAAriZXoenw4cMqV65ctvYyZcro8OHDt1wUAACAq8lVaAoKCtLWrVuztW/ZskXFixe/5aIAAABcTa5C03PPPafevXtrxYoVysjIUEZGhpYvX64+ffqoQ4cOeV0jAACA0+Xq03Nvv/22Dh48qCZNmsjD4/IQmZmZ6ty5M3OaAADAHSlXocnT01OzZ8/W22+/rS1btsjHx0fVqlVTmTJl8ro+AAAAl5Cr0JTlvvvu03333ZdXtQAAALisXIWmjIwMxcXFadmyZTpx4oQyMzMdli9fvjxPigMAAHAVuQpNffr0UVxcnFq2bKmqVavKZrPldV0AAAAuJVeh6csvv9RXX32lFi1a5HU9AAAALilXtxzw9PTUvffem9e1AAAAuKxchaYBAwZowoQJMgwjr+sBAABwSbm6PPfzzz9rxYoVWrRoke6//34VKlTIYfm3336bJ8UBAAC4ilyFpsDAQD399NN5XQsAAIDLylVomjZtWl7XAQAA4NJyNadJki5duqSlS5fqk08+0dmzZyVJx44dU2pqap4VBwAA4Cpydabp0KFDatasmQ4fPqz09HQ9/vjj8vPz05gxY5Senq4pU6bkdZ0AAABOlaszTX369FGtWrV0+vRp+fj4mO1PP/20li1blmfFAQAAuIpcnWn66aeftGbNGnl6ejq0ly1bVn/88UeeFAYAAOBKcnWmKTMzUxkZGdnajx49Kj8/v1suCgAAwNXkKjQ1bdpUH3zwgfncZrMpNTVVw4cP56tVAADAHSlXl+fGjRunyMhIValSRefPn1fHjh21b98+lShRQl988UVe1wgAAOB0uQpNpUuX1pYtW/Tll19q69atSk1NVffu3dWpUyeHieEAAAB3ilyFJkny8PDQ888/n5e1AAAAuKxchab//e9/113euXPnXBUDAADgqnIVmvr06ePw/OLFizp37pw8PT1VuHBhQhMAALjj5OrTc6dPn3Z4pKamas+ePapfvz4TwQEAwB0p1989d7UKFSrovffey3YWCgAA4E6QZ6FJujw5/NixY3k5pP744w89//zzKl68uHx8fFStWjWtX7/eXG4YhoYNG6ZSpUrJx8dHERER2rdvn8MYp06dUqdOneTv76/AwEB179492xcLb926VQ0aNJC3t7fCwsI0duzYPN0PAABQsOVqTtP8+fMdnhuGoePHj2vSpEmqV69enhQmXb4MWK9ePTVu3FiLFi1SyZIltW/fPhUtWtTsM3bsWE2cOFHTp09XuXLlNHToUEVGRmrnzp3y9vaWJHXq1EnHjx9XfHy8Ll68qK5du6pnz56aNWuWJMlut6tp06aKiIjQlClTtG3bNnXr1k2BgYHq2bNnnu0PAAAouGyGYRg3u5Kbm+MJKpvNppIlS+qxxx7TuHHjVKpUqTwpbvDgwfrll1/0008/5bjcMAyFhoZqwIABGjhwoCQpJSVFwcHBiouLU4cOHbRr1y5VqVJFiYmJqlWrliRp8eLFatGihY4eParQ0FBNnjxZb775ppKSkszv0xs8eLDmzZun3bt3W6rVbrcrICBAKSkp8vf3z4O9BwDc6Vq1cnYF+W/BAmdX4Ohm3r9z/d1zVz4yMjKUlJSkWbNm5Vlgki6f0apVq5batWunoKAgPfjgg/rss8/M5QcOHFBSUpIiIiLMtoCAANWpU0cJCQmSpISEBAUGBpqBSZIiIiLk5uamtWvXmn0aNmzo8AXEkZGR2rNnj06fPp1n+wMAAAquPJ3TlNf279+vyZMnq0KFCvrxxx/1yiuvqHfv3po+fbokKSkpSZIUHBzssF5wcLC5LCkpSUFBQQ7LPTw8VKxYMYc+OY1x5Taulp6eLrvd7vAAAAB3rlzNaerfv7/lvuPHj8/NJiRdPqNVq1Ytvfvuu5KkBx98UNu3b9eUKVMUFRWV63HzQmxsrEaOHOnUGgAAQP7JVWjatGmTNm3apIsXL6pixYqSpL1798rd3V0PPfSQ2c9ms91ScaVKlVKVKlUc2ipXrqxvvvlGkhQSEiJJSk5OdrgsmJycrBo1aph9Tpw44TDGpUuXdOrUKXP9kJAQJScnO/TJep7V52pDhgxxCI92u11hYWE3u4sAAKCAyNXluVatWqlhw4Y6evSoNm7cqI0bN+rIkSNq3LixnnjiCa1YsUIrVqzQ8uXLb6m4evXqac+ePQ5te/fuVZkyZSRJ5cqVU0hIiJYtW2Yut9vtWrt2rcLDwyVJ4eHhOnPmjDZs2GD2Wb58uTIzM1WnTh2zz+rVq3Xx4kWzT3x8vCpWrOjwSb0reXl5yd/f3+EBAADuXLkKTePGjVNsbKxDoChatKjeeecdjRs3Ls+K69evn3799Ve9++67+u233zRr1ix9+umnio6OlnT5TFbfvn31zjvvaP78+dq2bZs6d+6s0NBQtW7dWtLlM1PNmjVTjx49tG7dOv3yyy+KiYlRhw4dFBoaKknq2LGjPD091b17d+3YsUOzZ8/WhAkTbuoyJAAAuLPl6vKc3W7XyZMns7WfPHlSZ8+eveWisjz88MOaO3euhgwZolGjRqlcuXL64IMP1KlTJ7PP66+/rrS0NPXs2VNnzpxR/fr1tXjxYvMeTZI0c+ZMxcTEqEmTJnJzc1ObNm00ceJEc3lAQICWLFmi6Oho1axZUyVKlNCwYcO4RxMAADDl6j5NnTt31k8//aRx48apdu3akqS1a9fqtddeU4MGDcxPt91NuE8TAOBmcZ8m57uZ9+9cnWmaMmWKBg4cqI4dO5rzgDw8PNS9e3e9//77uRkSAADApeXqTFOWtLQ0/f7775Kk8uXLy9fXN88KK2g40wQAuFmcaXK+235H8CzHjx/X8ePHVaFCBfn6+uoW8hcAAIBLy1Vo+uuvv9SkSRPdd999atGihY4fPy5J6t69uwYMGJCnBQIAALiCXIWmfv36qVChQjp8+LAKFy5stj/77LNavHhxnhUHAADgKnI1EXzJkiX68ccfVbp0aYf2ChUq6NChQ3lSGAAAgCvJ1ZmmtLQ0hzNMWU6dOiUvL69bLgoAAMDV5Co0NWjQQP/73//M5zabTZmZmRo7dqwaN26cZ8UBAAC4ilxdnhs7dqyaNGmi9evX68KFC3r99de1Y8cOnTp1Sr/88kte1wgAAOB0uTrTVLVqVe3du1f169fXU089pbS0ND3zzDPatGmTypcvn9c1AgAAON1Nn2m6ePGimjVrpilTpujNN9+8HTUBAAC4nJs+01SoUCFt3br1dtQCAADgsnJ1ee7555/X1KlT87oWAAAAl5WrieCXLl3Sf//7Xy1dulQ1a9bM9p1z48ePz5PiAAAAXMVNhab9+/erbNmy2r59ux566CFJ0t69ex362Gy2vKsOAADARdxUaKpQoYKOHz+uFStWSLr8tSkTJ05UcHDwbSkOAADAVdzUnCbDMByeL1q0SGlpaXlaEAAAgCvK1UTwLFeHKAAAgDvVTYUmm82Wbc4Sc5gAAMDd4KbmNBmGoS5duphfynv+/Hm9/PLL2T499+233+ZdhQAAAC7gpkJTVFSUw/Pnn38+T4sBAABwVTcVmqZNm3a76gAAAHBptzQRHAAA4G5BaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCgQIWm9957TzabTX379jXbzp8/r+joaBUvXlxFihRRmzZtlJyc7LDe4cOH1bJlSxUuXFhBQUF67bXXdOnSJYc+K1eu1EMPPSQvLy/de++9iouLy4c9AgAABUWBCU2JiYn65JNP9MADDzi09+vXTwsWLNCcOXO0atUqHTt2TM8884y5PCMjQy1bttSFCxe0Zs0aTZ8+XXFxcRo2bJjZ58CBA2rZsqUaN26szZs3q2/fvnrxxRf1448/5tv+AQAA11YgQlNqaqo6deqkzz77TEWLFjXbU1JSNHXqVI0fP16PPfaYatasqWnTpmnNmjX69ddfJUlLlizRzp07NWPGDNWoUUPNmzfX22+/rY8++kgXLlyQJE2ZMkXlypXTuHHjVLlyZcXExKht27b697//7ZT9BQAArqdAhKbo6Gi1bNlSERERDu0bNmzQxYsXHdorVaqke+65RwkJCZKkhIQEVatWTcHBwWafyMhI2e127dixw+xz9diRkZHmGDlJT0+X3W53eAAAgDuXh7MLuJEvv/xSGzduVGJiYrZlSUlJ8vT0VGBgoEN7cHCwkpKSzD5XBqas5VnLrtfHbrfr77//lo+PT7Ztx8bGauTIkbneLwAAULC49JmmI0eOqE+fPpo5c6a8vb2dXY6DIUOGKCUlxXwcOXLE2SUBAIDbyKVD04YNG3TixAk99NBD8vDwkIeHh1atWqWJEyfKw8NDwcHBunDhgs6cOeOwXnJyskJCQiRJISEh2T5Nl/X8Rn38/f1zPMskSV5eXvL393d4AACAO5dLh6YmTZpo27Zt2rx5s/moVauWOnXqZP67UKFCWrZsmbnOnj17dPjwYYWHh0uSwsPDtW3bNp04ccLsEx8fL39/f1WpUsXsc+UYWX2yxgAAAHDpOU1+fn6qWrWqQ5uvr6+KFy9utnfv3l39+/dXsWLF5O/vr169eik8PFyPPPKIJKlp06aqUqWKXnjhBY0dO1ZJSUl66623FB0dLS8vL0nSyy+/rEmTJun1119Xt27dtHz5cn311Vf6/vvv83eHAQCAy3Lp0GTFv//9b7m5ualNmzZKT09XZGSkPv74Y3O5u7u7Fi5cqFdeeUXh4eHy9fVVVFSURo0aZfYpV66cvv/+e/Xr108TJkxQ6dKl9Z///EeRkZHO2CUAAOCCbIZhGM4u4k5gt9sVEBCglJQU5jcBACxp1crZFeS/BQucXYGjm3n/LvBnmgAAcKa7MfjcrVx6IjgAAICrIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALCE0AAAAWEJoAAAAsIDQBAABYQGgCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACzwcHYBAAA4W6tWzq4ABQGhCQAA5JtbCagLFuRdHbnB5TkAAAALCE0AAAAWEJoAAAAscOnQFBsbq4cfflh+fn4KCgpS69attWfPHoc+58+fV3R0tIoXL64iRYqoTZs2Sk5Oduhz+PBhtWzZUoULF1ZQUJBee+01Xbp0yaHPypUr9dBDD8nLy0v33nuv4uLibvfuAQCAAsSlQ9OqVasUHR2tX3/9VfHx8bp48aKaNm2qtLQ0s0+/fv20YMECzZkzR6tWrdKxY8f0zDPPmMszMjLUsmVLXbhwQWvWrNH06dMVFxenYcOGmX0OHDigli1bqnHjxtq8ebP69u2rF198UT/++GO+7i8AAHBdNsMwDGcXYdXJkycVFBSkVatWqWHDhkpJSVHJkiU1a9YstW3bVpK0e/duVa5cWQkJCXrkkUe0aNEiPfHEEzp27JiCg4MlSVOmTNGgQYN08uRJeXp6atCgQfr++++1fft2c1sdOnTQmTNntHjxYku12e12BQQEKCUlRf7+/nm/8wCA24ZbDhQMt+PTczfz/u3SZ5qulpKSIkkqVqyYJGnDhg26ePGiIiIizD6VKlXSPffco4SEBElSQkKCqlWrZgYmSYqMjJTdbteOHTvMPleOkdUna4ycpKeny263OzwAAMCdq8CEpszMTPXt21f16tVT1apVJUlJSUny9PRUYGCgQ9/g4GAlJSWZfa4MTFnLs5Zdr4/dbtfff/+dYz2xsbEKCAgwH2FhYbe8jwAAwHUVmNAUHR2t7du368svv3R2KZKkIUOGKCUlxXwcOXLE2SUBAIDbqEDcETwmJkYLFy7U6tWrVbp0abM9JCREFy5c0JkzZxzONiUnJyskJMTss27dOofxsj5dd2Wfqz9xl5ycLH9/f/n4+ORYk5eXl7y8vG553wAAQMHg0meaDMNQTEyM5s6dq+XLl6tcuXIOy2vWrKlChQpp2bJlZtuePXt0+PBhhYeHS5LCw8O1bds2nThxwuwTHx8vf39/ValSxexz5RhZfbLGAAAAcOkzTdHR0Zo1a5a+++47+fn5mXOQAgIC5OPjo4CAAHXv3l39+/dXsWLF5O/vr169eik8PFyPPPKIJKlp06aqUqWKXnjhBY0dO1ZJSUl66623FB0dbZ4pevnllzVp0iS9/vrr6tatm5YvX66vvvpK33//vdP2HQAAuBaXvuWAzWbLsX3atGnq0qWLpMs3txwwYIC++OILpaenKzIyUh9//LF56U2SDh06pFdeeUUrV66Ur6+voqKi9N5778nD4/9nxpUrV6pfv37auXOnSpcuraFDh5rbsIJbDgBAwcUtBwoGZ99ywKVDU0FCaAKAgovQVDA4OzS59JwmAAAAV0FoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALDApb+wFwAAq/gqFNxunGkCAACwgNAEAABgAaEJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCQAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFjg4ewCAADI0qqVsysAro0zTQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMACQhMAAIAFhCYAAAALuLklACBPcYNK3Kk40wQAAGABoQkAAMACQhMAAIAFhCYAAAALmAgOAMiGydxAdpxpAgAAsIAzTQBwh+JsEZC3ONMEAABgAaEJAADAAi7PAYAL4xIb4DoITVf56KOP9P777yspKUnVq1fXhx9+qNq1azu7LAAFGMEHuDNwee4Ks2fPVv/+/TV8+HBt3LhR1atXV2RkpE6cOOHs0gAAgJPZDMMwnF2Eq6hTp44efvhhTZo0SZKUmZmpsLAw9erVS4MHD77uuna7XQEBAUpJSZG/v39+lAsgH3G2CHC+BQvyfsybef/m8tz/uXDhgjZs2KAhQ4aYbW5uboqIiFBCQoITKwPuPAQQAAURoen//Pnnn8rIyFBwcLBDe3BwsHbv3p2tf3p6utLT083nKSkpki4nVtyd2rd3dgUAcGe7HW+xWe/bVi68EZpyKTY2ViNHjszWHhYW5oRqAAC48wUE3L6xz549q4AbbIDQ9H9KlCghd3d3JScnO7QnJycrJCQkW/8hQ4aof//+5vPMzEydOnVKxYsXl81mu+31Xo/dbldYWJiOHDlyV86vutv3X+IYSBwDiWNwt++/xDGQbnwMDMPQ2bNnFRoaesOxCE3/x9PTUzVr1tSyZcvUunVrSZeD0LJlyxQTE5Otv5eXl7y8vBzaAgMD86FS6/z9/e/aHxKJ/Zc4BhLHQOIY3O37L3EMpOsfgxudYcpCaLpC//79FRUVpVq1aql27dr64IMPlJaWpq5duzq7NAAA4GSEpis8++yzOnnypIYNG6akpCTVqFFDixcvzjY5HAAA3H0ITVeJiYnJ8XJcQeLl5aXhw4dnu3x4t7jb91/iGEgcA4ljcLfvv8QxkPL2GHBzSwAAAAv4GhUAAAALCE0AAAAWEJoAAAAsIDQBAABYQGi6g6xevVqtWrVSaGiobDab5s2b5+yS8lVsbKwefvhh+fn5KSgoSK1bt9aePXucXVa+mjx5sh544AHzJm7h4eFatGiRs8tymvfee082m019+/Z1din5ZsSIEbLZbA6PSpUqObusfPfHH3/o+eefV/HixeXj46Nq1app/fr1zi4r35QtWzbb68Bmsyk6OtrZpeWLjIwMDR06VOXKlZOPj4/Kly+vt99+29L3y10Ptxy4g6Slpal69erq1q2bnnnmGWeXk+9WrVql6OhoPfzww7p06ZLeeOMNNW3aVDt37pSvr6+zy8sXpUuX1nvvvacKFSrIMAxNnz5dTz31lDZt2qT777/f2eXlq8TERH3yySd64IEHnF1Kvrv//vu1dOlS87mHx931q/706dOqV6+eGjdurEWLFqlkyZLat2+fihYt6uzS8k1iYqIyMjLM59u3b9fjjz+udu3aObGq/DNmzBhNnjxZ06dP1/3336/169era9euCggIUO/evXM97t31k3SHa968uZo3b+7sMpxm8eLFDs/j4uIUFBSkDRs2qGHDhk6qKn+1atXK4fno0aM1efJk/frrr3dVaEpNTVWnTp302Wef6Z133nF2OfnOw8Mjx+/MvFuMGTNGYWFhmjZtmtlWrlw5J1aU/0qWLOnw/L333lP58uXVqFEjJ1WUv9asWaOnnnpKLVu2lHT5zNsXX3yhdevW3dK4XJ7DHSslJUWSVKxYMSdX4hwZGRn68ssvlZaWpvDwcGeXk6+io6PVsmVLRUREOLsUp9i3b59CQ0P1z3/+U506ddLhw4edXVK+mj9/vmrVqqV27dopKChIDz74oD777DNnl+U0Fy5c0IwZM9StWzenf6F8fqlbt66WLVumvXv3SpK2bNmin3/++ZZPLHCmCXekzMxM9e3bV/Xq1VPVqlWdXU6+2rZtm8LDw3X+/HkVKVJEc+fOVZUqVZxdVr758ssvtXHjRiUmJjq7FKeoU6eO4uLiVLFiRR0/flwjR45UgwYNtH37dvn5+Tm7vHyxf/9+TZ48Wf3799cbb7yhxMRE9e7dW56enoqKinJ2eflu3rx5OnPmjLp06eLsUvLN4MGDZbfbValSJbm7uysjI0OjR49Wp06dbmlcQhPuSNHR0dq+fbt+/vlnZ5eS7ypWrKjNmzcrJSVFX3/9taKiorRq1aq7IjgdOXJEffr0UXx8vLy9vZ1djlNc+Zf0Aw88oDp16qhMmTL66quv1L17dydWln8yMzNVq1Ytvfvuu5KkBx98UNu3b9eUKVPuytA0depUNW/eXKGhoc4uJd989dVXmjlzpmbNmqX7779fmzdvVt++fRUaGnpLrwFCE+44MTExWrhwoVavXq3SpUs7u5x85+npqXvvvVeSVLNmTSUmJmrChAn65JNPnFzZ7bdhwwadOHFCDz30kNmWkZGh1atXa9KkSUpPT5e7u7sTK8x/gYGBuu+++/Tbb785u5R8U6pUqWx/JFSuXFnffPONkypynkOHDmnp0qX69ttvnV1Kvnrttdc0ePBgdejQQZJUrVo1HTp0SLGxsYQmQJIMw1CvXr00d+5crVy58q6b+HktmZmZSk9Pd3YZ+aJJkybatm2bQ1vXrl1VqVIlDRo06K4LTNLlSfG///67XnjhBWeXkm/q1auX7XYje/fuVZkyZZxUkfNMmzZNQUFB5oTou8W5c+fk5uY4bdvd3V2ZmZm3NC6h6Q6Smprq8NfkgQMHtHnzZhUrVkz33HOPEyvLH9HR0Zo1a5a+++47+fn5KSkpSZIUEBAgHx8fJ1eXP4YMGaLmzZvrnnvu0dmzZzVr1iytXLlSP/74o7NLyxd+fn7Z5rD5+vqqePHid83ctoEDB6pVq1YqU6aMjh07puHDh8vd3V3PPfecs0vLN/369VPdunX17rvvqn379lq3bp0+/fRTffrpp84uLV9lZmZq2rRpioqKuutuO9GqVSuNHj1a99xzj+6//35t2rRJ48ePV7du3W5tYAN3jBUrVhiSsj2ioqKcXVq+yGnfJRnTpk1zdmn5plu3bkaZMmUMT09Po2TJkkaTJk2MJUuWOLssp2rUqJHRp08fZ5eRb5599lmjVKlShqenp/GPf/zDePbZZ43ffvvN2WXluwULFhhVq1Y1vLy8jEqVKhmffvqps0vKdz/++KMhydizZ4+zS8l3drvd6NOnj3HPPfcY3t7exj//+U/jzTffNNLT029pXJth3OLtMQEAAO4C3KcJAADAAkITAACABYQmAAAACwhNAAAAFhCaAAAALCA0AQAAWEBoAgAAsIDQBAAAYAGhCUCB1qVLF9lstmyPZs2aWVp/5cqVstlsOnPmzO0tFECBd3d9GQ2AO1KzZs00bdo0hzYvL6883caFCxfk6emZp2MCKFg40wSgwPPy8lJISIjDo2jRopIkm82m//znP3r66adVuHBhVahQQfPnz5ckHTx4UI0bN5YkFS1aVDabTV26dJEkPfroo4qJiVHfvn1VokQJRUZGSpJWrVql2rVry8vLS6VKldLgwYN16dIls5as9WJiYhQQEKASJUpo6NChyvrGqlGjRuX45cE1atTQ0KFDb9sxAnDrCE0A7ngjR45U+/bttXXrVrVo0UKdOnXSqVOnFBYWpm+++UaStGfPHh0/flwTJkww15s+fbo8PT31yy+/aMqUKfrjjz/UokULPfzww9qyZYsmT56sqVOn6p133nHY3vTp0+Xh4aF169ZpwoQJGj9+vP7zn/9Ikrp166Zdu3YpMTHR7L9p0yZt3bpVXbt2zYejASDX8uLbhAHAWaKiogx3d3fD19fX4TF69GjDMAxDkvHWW2+Z/VNTUw1JxqJFiwzDMIwVK1YYkozTp087jNuoUSPjwQcfdGh74403jIoVKxqZmZlm20cffWQUKVLEyMjIMNerXLmyQ59BgwYZlStXNp83b97ceOWVV8znvXr1Mh599NFbPBIAbjfONAEo8Bo3bqzNmzc7PF5++WVz+QMPPGD+29fXV/7+/jpx4sQNx61Zs6bD8127dik8PFw2m81sq1evnlJTU3X06FGz7ZFHHnHoEx4ern379ikjI0OS1KNHD33xxRc6f/68Lly4oFmzZqlbt243v+MA8hUTwQEUeL6+vrr33nuvubxQoUIOz202mzIzMy2Nezu0atVKXl5emjt3rjw9PXXx4kW1bdv2tmwLQN4hNAG4q2V9Ii7rLND1VK5cWd98840MwzDPJP3yyy/y8/NT6dKlzX5r1651WO/XX39VhQoV5O7uLkny8PBQVFSUpk2bJk9PT3Xo0EE+Pj55tUsAbhNCE4ACLz09XUlJSQ5tHh4eKlGixA3XLVOmjGw2mxYuXKgWLVrIx8dHRYoUybHvq6++qg8++EC9evVSTEyM9uzZo+HDh6t///5yc/v/sx0OHz6s/v3766WXXtLGjRv14Ycfaty4cQ5jvfjii6pcubKky8ELgOsjNAEo8BYvXqxSpUo5tFWsWFG7d+++4br/+Mc/NHLkSA0ePFhdu3ZV586dFRcXd82+P/zwg1577TVVr15dxYoVU/fu3fXWW2859OvcubP+/vtv1a5dW+7u7urTp4969uzp0KdChQqqW7euTp06pTp16tzcDgNwCpth/N/NQwAAt+zRRx9VjRo19MEHH1y3n2EYqlChgl599VX1798/f4oDcEs40wQA+ezkyZP68ssvlZSUxL2ZgAKE0AQA+SwoKEglSpTQp59+at65HIDr4/IcAACABdzcEgAAwAJCEwAAgAWEJgAAAAsITQAAABYQmgAAACwgNAEAAFhAaAIAALCA0AQAAGABoQkAAMCC/wcde4xrI48P3QAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()  # Convert images to tensors\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "def load_cifar10(data_path):\n",
        "    train_dataset = datasets.CIFAR10(root=data_path, train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.CIFAR10(root=data_path, train=False, download=True, transform=transform)\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "# Compute Scenario Complexity using Pixel Entropy\n",
        "def compute_scenario_complexity(dataset):\n",
        "    entropy_values = []\n",
        "    for img, _ in dataset:\n",
        "        img = torch.mean(img, dim=0).numpy() * 255  # Convert to grayscale\n",
        "        hist = cv2.calcHist([img.astype(np.uint8)], [0], None, [256], [0, 256])\n",
        "        hist = hist / np.sum(hist)  # Normalize histogram\n",
        "        entropy = -np.sum(hist * np.log2(hist + 1e-10))  # Compute entropy\n",
        "        entropy_values.append(entropy)\n",
        "\n",
        "    beta = np.mean(entropy_values) / np.log2(256)  # Normalize to [0,1]\n",
        "    return beta, entropy_values\n",
        "\n",
        "# Compute Channel Importance\n",
        "def compute_channel_importance(model, dataloader, beta, T=1):\n",
        "    importance_scores = {}\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            X = batch[0].to(next(model.parameters()).device)\n",
        "            if X.shape[1] == 1:\n",
        "                X = X.repeat(1, 3, 1, 1)  # Convert grayscale to RGB\n",
        "\n",
        "            for name, layer in model.named_modules():\n",
        "                if isinstance(layer, torch.nn.Conv2d):\n",
        "                    try:\n",
        "                        if X.shape[1] != layer.weight.shape[1]:\n",
        "                            continue\n",
        "                        X = layer(X)\n",
        "\n",
        "                        # Compute importance\n",
        "                        mean_per_channel = torch.mean(X, dim=0, keepdim=True)\n",
        "                        dynamic_importance = torch.mean((X - mean_per_channel) ** 2, dim=[0, 2, 3])\n",
        "                        static_importance = torch.norm(layer.weight, p=1, dim=[1, 2, 3])\n",
        "                        alpha = (beta ** 0.5) / T\n",
        "                        channel_importance = (1 - alpha) * dynamic_importance + alpha * static_importance\n",
        "\n",
        "                        importance_scores[name] = channel_importance.cpu().numpy()\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing layer {name}: {e}\")\n",
        "            break  # Process one batch for efficiency\n",
        "\n",
        "    return importance_scores\n",
        "\n",
        "# Load dataset\n",
        "data_path = \"./cifar10_data\"\n",
        "train_dataset, test_dataset = load_cifar10(data_path)\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=False)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False)  # ✅ Added for evaluation\n",
        "\n",
        "# Compute scenario complexity\n",
        "beta, entropy_values = compute_scenario_complexity(train_dataset)\n",
        "print(f\"Scenario Complexity (Beta): {beta}\")\n",
        "\n",
        "# Load pre-trained SSD-VGG model\n",
        "model = models.detection.ssd300_vgg16(pretrained=True)\n",
        "\n",
        "# Compute channel importance using **test dataloader**\n",
        "importance_scores = compute_channel_importance(model, test_dataloader, beta, T=1)  # ✅ Uses test data\n",
        "print(\"Channel Importance Scores Computed\")\n",
        "\n",
        "# Visualize entropy distribution\n",
        "plt.hist(entropy_values, bins=30, color='blue', alpha=0.7)\n",
        "plt.xlabel(\"Entropy\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Entropy Distribution in CIFAR-10 Dataset\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hJMgShX3PdEf"
      },
      "outputs": [],
      "source": [
        "# Load only 5 samples for debugging\n",
        "small_dataset = torch.utils.data.Subset(dataset, range(2))\n",
        "small_dataloader = torch.utils.data.DataLoader(small_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPFCT3ppPu1x",
        "outputId": "57d50e8c-1b82-4016-8769-9830de82a816"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer: backbone.features.0, Channel Importance: [6.955693  3.0487685 2.3222566 6.35076   4.6440716 4.2836943 3.1135821\n",
            " 3.7741716 4.8957515 3.0291765 7.2288127 3.329373  5.718876  2.9637375\n",
            " 5.2181816 3.6864438 2.9834185 2.8544497 4.009539  3.392775  4.3215656\n",
            " 5.0850625 2.8925276 5.366357  1.505572  3.4248714 6.0887284 3.0954971\n",
            " 5.4188056 3.1895573 3.876362  2.8271956 1.698621  3.0436163 5.8824897\n",
            " 5.52206   2.5829298 6.243795  3.162048  2.5443115 2.2626975 6.028619\n",
            " 3.608874  5.086574  2.9769177 8.0482025 3.543008  6.525727  4.5260086\n",
            " 4.9901104 2.5687962 7.320328  2.4676366 4.2029457 4.5307765 2.639919\n",
            " 3.0731094 4.820191  5.925259  4.283282  5.3123016 2.7409127 6.8774323\n",
            " 5.2943306]\n",
            "Layer: backbone.features.2, Channel Importance: [20.466906  12.840576  13.6496935 14.05227   14.129964  19.780476\n",
            " 16.418573  10.41596   13.682192   8.984833  20.131191  11.86918\n",
            " 18.874746  21.474947   6.9331694 13.67511   13.549175  18.233335\n",
            " 20.859634  22.289572  20.090181  12.629763   8.436704   7.810802\n",
            " 15.692555   8.47586   19.438467  18.560287  20.988134  21.520159\n",
            " 20.181211  11.956386  17.202795  19.165071  17.577538  19.55881\n",
            " 11.232182  19.215714  10.500883   7.670342  17.454466  14.842602\n",
            "  9.398622  13.072815  14.076875  19.169977  14.056248  20.8129\n",
            " 11.081133  14.292434  18.240759  11.0749    19.463541  18.256445\n",
            " 12.162227  19.96245   17.185137  18.410524  15.285784  22.857304\n",
            " 18.320148  11.317372  19.078468  12.81059  ]\n",
            "Layer: backbone.features.5, Channel Importance: [ 7.5036416 10.735398   8.438253  17.62915   13.227185  13.055365\n",
            " 14.72202   14.01278   11.897896  14.041457  12.551486  15.960956\n",
            " 17.509487  10.430308  14.436722  13.486785  14.431979  16.98627\n",
            " 15.931725  14.694894  14.681261  13.184108   9.83838   16.816072\n",
            " 15.676271  14.362116  14.960358   6.5478945  9.966978  13.942733\n",
            " 15.704464  10.295879  18.066898  12.200591   8.9327     9.485572\n",
            " 11.671878  12.343001  12.2199     8.412832   9.233826  15.589613\n",
            " 13.869891   5.0307546 13.558025  15.978641  15.300521  14.147679\n",
            " 14.7511     6.0821714  9.405722   9.018207   9.485301   6.6252127\n",
            " 17.937124  14.52192   16.808584   8.932055  13.441039   9.495868\n",
            " 10.861031  15.635784  10.957336  11.22984   14.982821  17.52138\n",
            " 13.801248   9.111768  11.250045   6.392185  17.79628   11.965681\n",
            " 14.606914  16.427414   8.152045  10.187097  16.702108   7.385959\n",
            " 13.628066  13.985913  13.365151  12.8894615 17.207186  15.734381\n",
            " 10.408854  14.112652  16.953697  11.223996  18.709677   8.103967\n",
            "  8.672057   9.208114   6.1390467 14.877227  11.736361   9.17042\n",
            " 15.67317   16.096943  13.750419  12.405458   6.19      13.06897\n",
            " 16.57567   15.110303  14.97496   10.466276  12.685505  17.11037\n",
            "  8.842497  14.715052  11.993684  12.928104  10.867117  14.261016\n",
            " 17.926012   7.5436373 15.846872  12.829556   7.928687   8.766937\n",
            " 10.803557   6.171466  15.974346   4.3126464  8.602081  16.689814\n",
            " 15.313114  16.10688  ]\n",
            "Layer: backbone.features.7, Channel Importance: [17.168137 21.700193 23.50783  16.595068 15.934091 25.72592  16.669163\n",
            " 19.652401 19.853437 21.333172 24.295212 17.709429 28.479322 18.24183\n",
            " 24.404716 19.184032 23.718777 17.38404  20.292599 22.749243 17.04407\n",
            " 19.88046  21.19911  17.634335 25.642822 18.821503 25.324606 20.234428\n",
            " 20.586117 17.983406 22.38745  19.08944  13.345976 23.504204 20.393118\n",
            " 17.018435 21.321358 21.917727 18.027372 17.525003 15.700207 23.648825\n",
            " 28.27344  16.261398 20.930538 16.594511 16.623198 24.492556 15.9901\n",
            " 19.721537 14.488416 19.018847 18.629326 16.585524 24.312862 29.201502\n",
            " 24.02573  18.389801 19.763699 24.54941  19.985191 20.905725 15.332911\n",
            " 16.652533 18.510439 15.027542 20.827438 17.219692 21.25088  20.692827\n",
            " 18.70635  21.90432  17.352255 16.067432 23.94383  21.337465 14.317841\n",
            " 16.12855  16.335867 21.35763  21.72077  23.566444 18.259535 20.446253\n",
            " 23.295923 19.597378 14.150462 19.334969 21.075308 21.615492 15.959235\n",
            " 17.151627 22.337042 23.995684 14.670441 22.432705 23.426674 14.88431\n",
            " 23.533808 26.761076 21.981094 25.013128 18.664589 23.73423  22.09571\n",
            " 20.480791 14.671322 22.66968  20.4298   22.845455 20.332977 21.491209\n",
            " 12.951135 28.116741 21.601934 17.591839 24.011618 17.509073 12.237576\n",
            " 19.037296 17.698732 22.666235 22.13626  18.008045 12.444376 21.362907\n",
            " 17.190722 23.261024]\n",
            "Layer: backbone.features.10, Channel Importance: [14.608388  15.656654  14.514847  13.951273  12.845602  15.375888\n",
            " 16.29547   17.037941  14.317195  15.878516  14.59734   18.561657\n",
            " 18.299213  15.172632  18.159836  15.872571  14.543092  18.948195\n",
            " 10.4961815 15.314163  14.641877  14.812783  18.202524  16.105196\n",
            " 14.564425  18.091108  17.809366  14.165671  13.44864   15.828376\n",
            " 14.632906  12.739632  17.516573  15.166561  13.459529  18.115944\n",
            " 10.63989   13.144918  12.332716  14.69326   15.36196   17.494741\n",
            " 12.56745   13.095217  12.486942  13.486567  19.214804  11.554223\n",
            " 13.645153  13.153284  17.940413  18.78509   16.408152  13.996213\n",
            " 15.916785  18.862923  13.156273  13.2939005 14.63764   19.542389\n",
            " 16.756987  13.153771  16.926992  15.918818  13.851748  15.033221\n",
            " 17.936264  15.236801  14.290165  18.776035  15.168189  17.422503\n",
            " 14.820724  13.735255  13.14025   15.564472  16.203781  18.486097\n",
            " 16.451332  18.888075  18.697699  18.180851  14.192814  16.839224\n",
            " 16.637812  16.481441  12.835443  13.433836  16.547068  12.935296\n",
            " 17.427067  17.495947  17.002577  17.042704  15.65583   12.906089\n",
            " 13.946558  18.01121   17.185444  17.572863  15.552599  17.022741\n",
            " 15.680154  13.171268  14.192052  15.8765135 14.207312  13.718105\n",
            " 18.463331  13.516606  11.9334545 17.363373  14.242907  14.346399\n",
            " 14.812744  17.488733  15.868967  15.349328  18.56741   15.29367\n",
            " 15.256658  15.326147  15.531525  14.986221  17.72393   18.531712\n",
            " 16.047533  14.485651  15.646687  14.516307  14.229809  17.397835\n",
            " 14.920357  13.631259  14.55213   14.562264  17.725197  17.206608\n",
            " 17.155556  14.8161745 17.141743  14.423787  17.150698  20.465431\n",
            " 15.058005  15.0608225 17.167374  17.046043  19.304134  23.194696\n",
            " 13.120962  16.793253  14.043163  18.55375   12.985105  16.82442\n",
            " 18.36147   13.485524  12.502218  11.150956  18.28968   15.12598\n",
            " 13.974068  16.520187  16.655354  13.9786005 17.39891   15.483123\n",
            " 18.395744  16.371569  15.964863  13.430525  14.456882  14.124245\n",
            " 15.892871  16.256784  16.65976   16.951853  14.951815  18.05736\n",
            " 16.004309  17.883495  16.932707  14.571371  14.635229  13.974132\n",
            " 12.40218   17.04725   15.235847  19.117006  12.707845  15.5053005\n",
            " 17.854582  18.146059  15.325266  13.173102  15.6927595 14.795773\n",
            " 15.88632   14.900716  19.456482  13.100877  15.16617   15.746743\n",
            " 17.983627  14.4318285 16.419086  18.361647  17.277777  15.169872\n",
            " 19.954191  18.855104  16.124125  17.087164  19.045914  14.060945\n",
            " 17.33604   16.39434   13.93788   15.800543  17.58471   15.502874\n",
            " 20.19653   17.830444  17.854446  16.65673   15.249441  17.369844\n",
            " 14.040467  16.704252  16.485212  17.51764   18.48734   13.828639\n",
            " 14.985387  15.1668415 20.484215  13.910596  16.098299  12.054867\n",
            " 15.3295145 17.112137  12.217912  18.211962  18.863384  18.323835\n",
            " 13.952279  16.382095  16.910582  18.224657  14.158845  14.843451\n",
            " 21.72927   18.058453  19.90707   13.816946 ]\n",
            "Layer: backbone.features.12, Channel Importance: [30.2112   27.539696 26.375383 22.447142 18.78164  24.4276   25.53255\n",
            " 26.342598 28.008146 26.367004 29.556105 28.22178  25.63151  23.864727\n",
            " 26.18116  27.418585 26.073416 27.170862 26.60192  24.316296 23.43669\n",
            " 27.351498 26.939627 28.036518 23.304977 23.2637   24.46979  22.527752\n",
            " 18.734903 25.469107 24.521463 22.174305 21.612728 25.331566 27.682152\n",
            " 25.60775  29.642115 24.705843 23.68437  24.108181 25.510015 21.847313\n",
            " 23.993032 29.825447 25.988386 28.118067 29.466684 23.28895  25.729624\n",
            " 23.088173 22.194101 25.161913 25.489393 26.935696 27.489498 24.353064\n",
            " 28.66913  31.582008 22.510614 22.665997 27.587383 23.23115  24.03098\n",
            " 22.512424 33.137318 19.473732 26.200613 27.034945 24.07402  23.797543\n",
            " 29.595675 25.421883 29.767118 24.752666 31.379065 24.765047 24.611275\n",
            " 21.756086 30.279293 15.543224 25.4036   27.256025 22.390339 29.724579\n",
            " 24.52661  20.13176  26.892244 22.673483 26.613342 24.9761   22.181135\n",
            " 24.636679 20.466475 27.390987 20.105589 25.882708 23.654173 24.855705\n",
            " 27.963598 25.421158 30.163866 21.670322 19.568773 28.248335 24.26745\n",
            " 21.867842 24.651497 29.501364 19.933475 19.540953 24.625008 28.295647\n",
            " 24.902342 28.5407   26.16315  24.981663 24.268803 25.689268 26.253141\n",
            " 22.857086 20.110659 24.010384 22.840256 22.089222 20.44687  24.872013\n",
            " 25.76769  27.210812 36.199787 20.667393 21.76552  23.607357 25.598663\n",
            " 27.951504 28.42457  25.129791 25.24297  21.280779 26.385344 27.20789\n",
            " 21.588552 23.17492  19.788946 21.449932 25.64571  24.146976 16.624067\n",
            " 29.043467 24.535746 22.713118 26.091562 24.542673 24.15524  19.773987\n",
            " 29.88428  25.609667 20.868849 26.411505 26.888065 24.060413 24.43847\n",
            " 22.26328  23.166685 24.0996   26.864944 20.043537 25.619066 24.712545\n",
            " 25.540531 29.060318 23.612577 28.422766 26.06089  25.242565 30.183283\n",
            " 24.949078 26.760307 21.94506  25.72401  22.873299 22.415234 26.972557\n",
            " 26.907396 23.127968 24.389095 24.116537 29.314034 20.217716 25.084574\n",
            " 20.571413 27.172583 23.595648 25.088718 26.573013 19.85488  22.20466\n",
            " 27.018204 17.967054 25.205402 29.900328 25.084362 27.31527  28.845938\n",
            " 23.228264 22.897024 21.903181 29.204567 21.5929   22.284794 22.363958\n",
            " 21.465023 28.913445 27.538748 29.432127 25.745514 22.786263 12.714269\n",
            " 29.099728 26.019533 26.976713 24.961235 21.257877 26.170198 25.119389\n",
            " 25.235275 23.360355 23.703085 23.519611 24.188112 30.70276  29.860825\n",
            " 23.298721 27.96774  28.031    23.582092 25.743565 29.120798 15.288011\n",
            " 23.04034  23.410406 24.570997 25.539818 25.980835 19.739738 22.994673\n",
            " 22.179743 27.27453  15.079263 20.669876 22.204865 29.975756 26.53008\n",
            " 33.394386 23.937447 22.165424 22.922243]\n",
            "Layer: backbone.features.14, Channel Importance: [27.019526 23.999462 25.292036 24.455952 25.875313 20.35854  21.83371\n",
            " 22.057919 25.348162 23.44222  27.448973 21.816074 27.756847 25.690023\n",
            " 23.127224 28.450743 26.407272 28.000673 24.123844 32.24281  26.87267\n",
            " 22.297028 27.979189 28.764065 24.088375 25.094282 31.331146 27.910994\n",
            " 26.672897 27.106266 22.012917 22.932623 25.108109 25.752258 25.25302\n",
            " 29.094902 24.464901 30.399378 26.861692 23.88307  24.778873 23.438854\n",
            " 26.885256 24.768576 23.431807 25.956385 24.433596 21.07738  27.031986\n",
            " 25.161844 25.681181 29.44079  24.829865 28.24623  16.717787 20.340212\n",
            " 26.120249 24.19535  23.742037 24.597282 27.651253 26.526419 22.104158\n",
            " 30.424274 26.348398 22.809679 26.028748 19.715254 21.1995   26.257795\n",
            " 23.902012 25.817293 22.8652   24.69153  26.813828 24.427588 24.019688\n",
            " 31.222557 25.950184 23.125114 23.595448 22.484362 30.970707 21.898014\n",
            " 25.661207 24.115927 26.659597 21.990246 28.238424 25.828918 27.917574\n",
            " 22.289392 28.173986 25.234993 25.736212 22.827173 23.964828 23.705675\n",
            " 24.266563 28.009254 25.32835  26.37977  24.94031  17.83133  20.828318\n",
            " 26.16921  29.424458 25.403275 23.935015 22.507425 27.495388 25.417973\n",
            " 32.85016  25.20176  23.44244  30.088785 28.908545 28.559557 24.825867\n",
            " 26.773785 23.384089 27.13154  23.653975 22.973854 22.437918 21.295353\n",
            " 27.804087 23.414644 29.299578 28.21521  28.458075 27.125874 23.695578\n",
            " 25.577133 23.478401 29.585453 25.48452  27.380701 25.67578  23.491726\n",
            " 25.775225 24.74499  22.642206 24.697931 24.07783  29.400951 24.968561\n",
            " 22.436731 20.901709 24.009214 25.612484 20.56576  25.431719 25.897604\n",
            " 24.129366 23.582916 25.800463 24.346245 23.735489 24.158865 24.943438\n",
            " 22.771545 23.634617 24.088148 22.940199 28.83299  26.112955 24.805304\n",
            " 23.153086 23.017805 23.727127 26.087107 30.012247 26.640928 30.883045\n",
            " 28.116114 33.077087 25.08439  24.196205 24.117094 25.558212 23.270184\n",
            " 24.154613 22.468    25.281286 24.528133 30.675066 28.66165  25.019955\n",
            " 22.528492 29.735838 24.617502 21.985584 28.676325 19.218134 28.304157\n",
            " 25.181541 23.446    22.070919 25.882923 31.243114 26.400019 24.602777\n",
            " 23.189161 31.280066 24.068132 24.78808  22.341715 27.097332 19.716948\n",
            " 29.793077 22.720665 24.167263 23.359703 22.877295 29.448576 27.41412\n",
            " 26.824398 27.090132 30.530787 23.807905 22.213064 27.799683 27.125353\n",
            " 27.370575 23.714867 26.019915 26.995243 29.339827 24.966276 22.330168\n",
            " 31.633684 25.53715  28.871958 23.612549 25.40843  26.084654 20.943802\n",
            " 25.475544 26.796589 24.347214 27.877224 23.562449 22.703547 29.271076\n",
            " 28.906033 24.806677 22.685785 26.55146  23.2639   24.41662  30.845047\n",
            " 21.36673  24.135567 22.979895 32.143814]\n",
            "Layer: backbone.features.17, Channel Importance: [21.156153 22.192081 20.772057 20.721949 20.708689 20.85942  23.961086\n",
            " 20.68468  20.135101 20.33095  19.79455  20.25726  19.672527 19.516464\n",
            " 21.13168  21.324259 18.885744 20.63864  21.220806 23.806301 20.36981\n",
            " 25.127998 20.785875 24.85877  21.662977 21.978058 20.38281  19.549221\n",
            " 20.409737 22.346231 19.492714 22.113531 18.429865 21.33348  21.408943\n",
            " 21.579971 21.883015 20.060553 20.256994 20.526676 21.329165 21.153416\n",
            " 21.621193 22.310293 21.948093 18.652042 21.138916 18.673471 20.997059\n",
            " 21.038588 21.611568 21.462082 22.335245 22.958902 21.642689 21.123888\n",
            " 22.177666 21.167887 19.856785 17.961855 23.480896 22.051954 25.910614\n",
            " 21.143085 21.777336 16.874006 21.867525 21.647066 19.683577 21.596935\n",
            " 18.817032 20.225689 22.814497 20.062046 20.717773 19.093586 18.41353\n",
            " 22.102684 22.35702  21.754833 19.463596 20.172985 21.051998 19.206757\n",
            " 24.251894 18.661913 20.169798 22.684664 15.684881 22.563244 27.435656\n",
            " 21.464893 19.792831 19.645649 21.305838 18.925375 21.400978 19.1494\n",
            " 25.270584 25.478947 20.777414 20.88716  23.261086 20.937044 18.593037\n",
            " 18.966133 19.15096  24.788637 20.941837 20.4759   21.254936 20.830177\n",
            " 21.806692 20.587273 23.440586 21.365982 19.212849 23.702724 21.026443\n",
            " 23.902477 18.064491 19.76052  23.5249   21.742977 23.290592 19.043083\n",
            " 21.247185 21.51003  22.67757  24.606482 19.409512 21.542015 21.068636\n",
            " 19.36116  17.537514 20.949162 21.741283 19.923227 20.305655 18.882246\n",
            " 22.945526 22.872295 22.658651 24.198532 20.938469 20.242184 21.920908\n",
            " 19.83469  23.628847 25.356602 18.949701 20.955801 20.391975 22.24143\n",
            " 21.854626 24.447432 22.149588 19.874279 20.357542 16.995197 18.723011\n",
            " 24.838755 21.393705 20.615381 19.014711 23.966743 25.517515 21.037464\n",
            " 22.27384  24.171541 22.660578 19.836086 18.745193 25.297283 19.297606\n",
            " 20.045074 20.421677 21.85469  20.778805 20.122818 20.894705 22.831944\n",
            " 19.655916 21.19728  19.759367 19.016708 21.681837 23.401758 21.979843\n",
            " 21.216593 24.189978 19.745789 23.347734 18.59014  19.465216 24.528778\n",
            " 18.433895 21.120611 20.510012 21.49603  22.560621 23.416256 19.820183\n",
            " 23.786783 23.864176 20.868116 21.58677  20.48242  21.007847 23.751047\n",
            " 24.153214 23.205715 18.700958 19.755743 20.372211 22.32758  20.341055\n",
            " 19.67827  19.270283 22.500853 19.866068 23.212805 20.923677 19.\n",
            " 24.583458 21.01057  26.15576  21.71073  20.077978 20.870808 22.711935\n",
            " 22.998617 22.483788 21.76368  21.834293 23.829647 20.776608 21.71563\n",
            " 20.235636 26.592901 23.26239  21.237787 22.173252 22.317053 23.230661\n",
            " 23.35578  22.223797 21.333893 23.295017 22.692802 24.250597 23.877888\n",
            " 21.23569  19.379414 19.46961  28.976776 19.658445 21.292753 24.211887\n",
            " 23.2429   22.045994 20.64584  29.130276 22.916111 19.196514 21.376905\n",
            " 21.21482  20.50361  22.592098 19.253809 23.970327 18.980087 22.62541\n",
            " 20.41826  26.992882 20.504429 21.775654 19.104786 23.786577 25.797062\n",
            " 23.796019 21.60884  21.480824 23.065733 21.357162 21.796577 18.511404\n",
            " 25.100142 20.416424 19.845806 21.556835 20.573809 22.18313  22.665476\n",
            " 23.43559  22.886282 22.370424 22.015055 20.28749  21.197544 17.722006\n",
            " 20.327784 18.59654  22.706175 21.687426 23.075003 22.601847 19.695784\n",
            " 20.958435 24.754148 21.88691  19.338427 20.216715 20.010193 19.725391\n",
            " 22.486826 20.247501 22.400242 22.027578 18.712801 22.819895 24.554277\n",
            " 21.710884 17.634293 22.057371 19.44446  21.042255 22.152596 23.360994\n",
            " 21.874588 22.252214 22.055897 21.567772 20.636543 20.904959 19.57405\n",
            " 21.347792 24.444725 21.171995 18.752499 23.696102 22.052986 22.94759\n",
            " 21.630226 23.725962 23.07169  25.151676 22.492891 22.851315 21.925095\n",
            " 20.98143  21.123167 18.570723 21.29079  21.960604 21.228062 22.053879\n",
            " 19.863218 20.310326 22.932007 20.3956   21.146088 22.559807 22.251942\n",
            " 23.900263 21.062843 22.633833 22.13513  18.492043 22.025629 21.296307\n",
            " 21.274204 22.663485 21.45101  20.874002 21.500914 21.456345 18.902102\n",
            " 23.74383  21.204336 19.89856  22.266773 18.279556 24.036907 18.58223\n",
            " 22.034107 21.613594 23.049486 21.339245 24.149326 20.916962 22.59602\n",
            " 20.384323 22.017933 18.937849 22.190445 20.700035 19.536041 21.836695\n",
            " 19.63895  23.00934  21.881645 21.643063 24.397535 24.024137 22.112028\n",
            " 19.029242 23.784964 21.596346 21.647072 20.716434 22.998348 23.250387\n",
            " 24.019135 21.526915 23.748766 23.008657 20.970005 21.191826 24.665197\n",
            " 20.11937  22.42175  22.34538  19.60567  20.749348 21.341335 18.944536\n",
            " 20.147852 23.652338 22.592852 18.605505 21.772991 20.471762 23.166956\n",
            " 19.325418 19.804918 18.645151 22.048231 19.06705  20.41128  23.202078\n",
            " 21.011284 21.920013 20.158619 26.476849 18.607523 21.823341 19.343966\n",
            " 19.446823 22.820122 20.72416  19.997772 24.696077 22.430244 19.165995\n",
            " 20.679232 22.507479 20.237204 23.277405 20.513489 20.021378 15.959715\n",
            " 21.839506 21.830254 21.221846 20.59589  21.660053 21.76786  21.538996\n",
            " 16.125803 20.554144 20.591297 22.912306 21.81843  21.024675 22.306643\n",
            " 23.462091 21.768267 17.779968 17.222963 19.130598 20.844242 18.561491\n",
            " 21.101759 20.511595 22.775297 22.093594 21.262629 25.318935 22.61407\n",
            " 29.701834 22.090029 24.08578  22.813866 21.342684 19.216766 21.577341\n",
            " 21.744303 21.964659 21.35643  22.302288 20.594084 22.365883 21.09832\n",
            " 20.642902 17.242147 18.625196 21.14861  19.882896 21.465616 22.394756\n",
            " 21.688282]\n",
            "Layer: backbone.features.19, Channel Importance: [34.95979  31.08589  32.876053 31.831472 32.438313 32.23429  30.215624\n",
            " 26.233932 30.768318 29.934675 29.17149  29.734709 36.245975 46.01322\n",
            " 37.816402 34.816345 37.579784 32.10138  31.109573 28.101105 30.965933\n",
            " 30.158422 38.690746 44.11334  28.600643 28.420858 29.958559 27.007265\n",
            " 32.204353 30.619802 28.304134 29.348328 32.672035 31.300697 38.349426\n",
            " 31.209623 34.967632 30.958305 25.477734 32.329308 29.40544  30.623161\n",
            " 30.036564 36.184315 34.380745 29.45438  29.385843 29.498114 33.51757\n",
            " 35.04075  28.238062 33.05674  28.44106  39.472393 31.078743 34.111618\n",
            " 28.804104 31.742382 33.320206 39.10102  32.904884 33.59834  28.709059\n",
            " 31.048674 29.610226 29.009233 26.674225 32.061317 30.668665 39.738785\n",
            " 34.944157 31.106756 32.89158  36.205257 40.892586 28.390425 33.49165\n",
            " 29.954252 35.99999  36.2224   32.340984 32.74359  32.53533  39.520393\n",
            " 40.055313 27.615604 28.970594 36.329372 36.524498 29.059883 34.891186\n",
            " 32.235275 33.397266 30.209688 26.328102 34.206116 25.629456 31.77025\n",
            " 31.708826 30.230005 30.40485  30.24005  29.867208 27.923672 35.204906\n",
            " 32.173695 37.255028 36.230083 34.61009  35.91582  31.610836 42.251728\n",
            " 29.592333 35.08667  32.10059  29.13104  28.996094 30.017233 30.020882\n",
            " 35.570347 31.324163 33.256104 35.09356  30.144022 32.54238  46.20697\n",
            " 36.091858 39.750153 35.953876 38.02424  29.247408 28.46596  27.725618\n",
            " 27.44145  27.380735 33.21342  32.57505  29.393799 35.116524 33.39018\n",
            " 28.253708 27.579569 30.942865 37.1666   36.603336 28.847599 30.90523\n",
            " 28.953367 38.407627 29.5712   39.10202  29.063938 30.908857 26.726242\n",
            " 27.745804 36.997    32.724693 28.467403 30.088924 37.296932 34.362206\n",
            " 33.983425 36.34146  30.919205 25.644438 31.863016 27.912266 35.42838\n",
            " 27.983776 29.087986 29.540901 27.898281 24.719692 28.402012 32.848934\n",
            " 31.277828 32.611866 29.237898 29.881865 31.371572 25.365158 32.61408\n",
            " 27.753483 30.607265 33.70622  29.277695 38.406624 31.59396  32.4599\n",
            " 27.123388 34.7242   30.670902 26.799177 31.215162 32.26052  30.805576\n",
            " 38.408813 28.090456 29.21132  35.202003 34.954975 31.8309   32.447197\n",
            " 37.76023  31.736765 30.542881 31.744318 31.801386 30.927647 31.870806\n",
            " 28.017017 27.988527 32.366726 33.704823 33.913387 30.977407 34.071083\n",
            " 28.345343 36.629856 38.186188 29.509632 29.612852 31.48689  29.516943\n",
            " 35.81311  35.63984  32.268177 32.045563 33.399227 30.68579  35.716328\n",
            " 30.645842 28.264492 32.64739  29.538855 34.487507 29.764593 36.6802\n",
            " 38.91463  31.531427 31.465395 31.217726 40.48847  36.399464 32.545086\n",
            " 29.80501  29.97544  29.493584 31.66675  30.901722 34.475246 30.797878\n",
            " 33.97061  31.648418 38.234673 27.333267 29.894516 37.681927 30.932678\n",
            " 31.36918  31.63571  32.23056  31.45566  31.731543 29.685696 32.19686\n",
            " 32.091618 39.47043  30.016973 35.01968  29.471218 36.619476 30.14875\n",
            " 33.2991   28.295523 30.113775 38.00985  31.092628 29.070345 29.169504\n",
            " 33.72015  35.362907 35.47828  26.655792 27.970127 33.931313 29.816837\n",
            " 29.902649 36.139496 32.653416 28.613308 33.763542 32.451283 27.265474\n",
            " 27.491976 31.711615 28.682148 28.921215 32.376293 28.992582 32.3069\n",
            " 30.768005 31.38985  30.853994 28.922401 31.123491 30.594032 29.851984\n",
            " 31.558092 30.422188 32.79761  29.647367 25.21579  25.33289  29.209911\n",
            " 28.895443 35.170704 32.532322 31.43653  33.309013 28.937042 29.331482\n",
            " 34.086754 31.440212 28.894869 28.537355 32.55175  30.997036 33.432026\n",
            " 32.09052  34.029457 31.400463 29.804407 38.780014 40.466637 29.125954\n",
            " 32.004635 31.857494 34.776947 29.208187 29.050209 28.964592 38.806034\n",
            " 30.58741  33.130806 31.579317 30.651434 30.826067 29.76567  28.845318\n",
            " 33.773117 29.565277 35.180492 40.478832 34.060833 34.116116 29.753267\n",
            " 28.246231 38.288494 35.234203 34.29113  45.175198 30.457235 34.38614\n",
            " 30.487673 35.700397 28.82871  25.445805 33.72081  29.231316 27.641712\n",
            " 30.791061 28.499794 30.24052  29.156872 35.834682 41.55159  41.529305\n",
            " 34.68323  37.95313  28.591175 32.700474 28.68903  31.399502 34.320023\n",
            " 35.467915 30.160978 34.617043 29.590147 36.945606 29.12259  27.867989\n",
            " 31.034351 32.42303  32.31059  32.94467  35.923443 29.853859 35.54589\n",
            " 30.040844 45.161068 44.50671  29.447258 29.740122 33.69685  30.796568\n",
            " 33.35224  32.21267  29.73599  37.636448 27.608404 31.252378 33.318047\n",
            " 31.843668 31.889286 29.01527  32.706444 39.09551  32.471607 32.26628\n",
            " 28.512516 36.855938 31.155107 26.406908 29.258532 34.282352 29.395178\n",
            " 33.317467 33.12851  32.36847  38.468914 34.215874 29.625496 31.066378\n",
            " 30.502134 32.594944 30.032812 31.572668 27.199816 27.918789 35.092987\n",
            " 33.138386 30.20814  28.01033  29.355648 34.86709  29.285616 37.154907\n",
            " 42.199455 35.712612 27.953531 31.713993 25.689825 30.980404 32.906063\n",
            " 33.087246 33.779255 37.878258 43.505196 37.425236 31.646784 31.13002\n",
            " 29.124184 25.72775  32.62383  39.95941  30.37969  29.08646  44.600605\n",
            " 30.454504 27.541037 33.77823  30.604298 26.981447 32.473667 32.15929\n",
            " 27.633451 33.614876 37.822575 34.475666 32.531815 26.74265  30.189638\n",
            " 36.637653 29.530321 30.936375 31.324238 34.490536 39.542114 30.82019\n",
            " 35.183437 40.884308 32.585754 28.175291 34.515965 32.331017 31.036503\n",
            " 33.24506  31.864328 27.38069  31.795135 30.981796 30.195494 32.506027\n",
            " 27.339489 27.582947 30.476812 29.066282 32.748226 29.829094 30.683258\n",
            " 28.49928 ]\n",
            "Layer: backbone.features.21, Channel Importance: [21.255594  24.606556  23.761688  36.257603  27.233713  22.377993\n",
            " 28.752522  28.51122   26.31166   24.267473  21.561094  30.821352\n",
            " 27.31699   21.583048  15.778663  22.704908  31.290455  27.853132\n",
            " 22.247448  23.271725  32.419384  24.277092  21.574175  30.415583\n",
            " 21.22733   27.610714  33.63802   24.452736  23.392624  27.392618\n",
            " 18.316002  22.16936   23.265112  24.60241   27.997183  25.173058\n",
            " 21.299017  21.346619  33.121338  26.100307  31.982477  31.290007\n",
            " 34.377068  31.433949  23.55277   23.114801  32.04957   20.102213\n",
            " 25.526003  24.611094  23.66974   28.50255   19.690771  24.273336\n",
            " 26.910355  22.868702  28.698229  30.784298  22.99462   25.519054\n",
            " 34.02569   24.89748   31.959156  22.551811  23.348051  27.238838\n",
            " 30.722143  30.370722  28.718498  27.25044   28.60987   21.88647\n",
            " 33.34714   26.612644  21.066023  23.188349  27.944399  27.349379\n",
            " 25.76631   23.40706   24.847033  20.094349  28.916399  21.459013\n",
            " 27.675575  23.529173  31.629156  23.447914  25.869589  29.1685\n",
            " 23.938944  25.479298  19.110289  25.917948  24.212046  24.561289\n",
            " 30.539478  23.083261  28.303102  25.041758  28.479212  35.131134\n",
            " 30.378456  23.938108  29.408031  39.775993  28.809439  23.13696\n",
            " 28.239422  28.422596  26.267267  28.957556  27.008097  21.818605\n",
            " 19.352718  29.273886  22.320463  21.1557    26.991728  26.288273\n",
            " 25.51766   20.291147  23.02719   24.986229  24.451227  27.560791\n",
            " 25.026218  23.326563  27.687746  26.020084  32.083157  23.519917\n",
            " 33.164085  36.10964   29.937508  27.247282  27.098158  23.957754\n",
            " 20.072115  21.609236  24.206207  25.277918  32.515003  41.21472\n",
            " 25.340178  28.026356  24.413906  38.481853  27.941645  21.535042\n",
            " 26.684855  13.3192625 36.05954   25.32195   37.34891   28.24068\n",
            " 22.377563  29.868053  24.293715  26.453737  31.416828  25.413647\n",
            " 28.147038  36.51397   30.151438  29.73296   28.628017  28.876163\n",
            " 25.68411   41.75531   21.826998  28.773518  32.6269    23.909042\n",
            " 24.713215  19.633667  20.355978  28.899021  28.051382  27.495304\n",
            " 20.441286  52.434307  32.646484  23.219954  22.187397  30.096079\n",
            " 28.647367  27.95807   18.199913  27.692451  28.623163  30.104628\n",
            " 28.97189   37.055298  37.029663  22.943815  25.667244  24.667866\n",
            " 22.389906  23.310034  26.55886   39.35395   20.278234  26.825218\n",
            " 27.231638  26.784132  29.975304  23.98768   35.466557  29.226067\n",
            " 20.5848    21.914818  26.375885  19.609438  22.001019  28.754597\n",
            " 33.938416  24.897758  27.90656   32.817276  25.949549  24.806961\n",
            " 28.25076   26.499662  32.59908   31.584389  46.45428   25.88888\n",
            " 31.677958  21.546242  27.94928   21.8954    24.098776  30.945045\n",
            " 29.119848  24.529566  24.816185  27.056484  20.373138  28.761843\n",
            " 46.746742  23.731688  26.066576  25.784618  25.930277  24.82335\n",
            " 23.533792  28.313202  25.803835  19.523195  26.309813  35.663315\n",
            " 26.014399  22.208725  28.004667  24.858719  34.834873  23.548811\n",
            " 27.56503   27.811146  31.213188  24.508818  28.216034  24.301065\n",
            " 34.93862   30.98736   25.790627  34.917923  28.271606  47.644707\n",
            " 13.574489  21.927813  33.360218  21.454065  24.893885  24.493189\n",
            " 25.50847   27.152178  59.1213    21.164797  25.495384  36.93223\n",
            " 27.881807  29.89459   27.572706  24.34392   31.11455   30.844437\n",
            " 26.572626  32.231308  21.994595  22.538029  25.001053  28.341225\n",
            " 22.95331   29.214756  25.489738  24.930323  26.097227  26.275331\n",
            " 28.218966  29.090502  27.317245  22.417149  24.353842  24.56767\n",
            " 24.321608  26.097385  28.556953  19.95633   24.986086  23.80886\n",
            " 25.960073  27.041695  22.21274   25.353886  24.94565   32.439228\n",
            " 20.119715  25.998306  20.970024  21.434212  25.436028  24.862682\n",
            " 28.43912   32.800156  21.926516  31.698917  26.364975  25.316332\n",
            " 19.10862   30.916807  33.22854   20.25101   30.960777  23.707699\n",
            " 29.593271  23.955969  27.911339  17.771687  26.467068  59.68784\n",
            " 25.05606   37.380447  31.113745  25.620922  29.056238  36.75416\n",
            " 30.281689  25.493927  28.187815  25.496529  23.562708  23.635788\n",
            " 24.541899  24.075207  30.626858  22.98957   27.315248  28.683723\n",
            " 21.39272   27.492716  33.097946  26.14089   25.091387  26.67188\n",
            " 25.966955  26.474112  28.737064  29.402914  26.138777  31.106792\n",
            " 28.94818   27.914595  28.54793   29.368849  31.6619    25.054642\n",
            " 25.842405  30.573986  25.925821  20.019295  27.430916  17.324335\n",
            " 36.425804  26.275652  26.138962  26.44177   27.73996   27.981918\n",
            " 28.846672  28.910383  23.475666  22.123346  25.354641  30.889355\n",
            " 24.647089  25.800222  37.117115  31.432905  22.636175  23.066637\n",
            " 23.181072  20.817642  20.871641  26.436398  29.054146  25.797232\n",
            " 32.609337  26.998865  23.915216  19.87094   29.270607  26.884344\n",
            " 26.62174   29.999212  25.310675  25.159344  22.994154  28.061586\n",
            " 25.0053    21.888216  42.106785  32.956463  29.900534  24.61892\n",
            " 36.70682   25.188705  18.590044  20.719614  21.827251  28.603634\n",
            " 27.183079  34.36348   22.416372  24.334755  22.789831  28.900015\n",
            " 31.006767  25.323952  30.704805  29.26187   21.572653  35.406403\n",
            " 26.490074  27.595327  21.858727  27.80067   21.656773  24.482481\n",
            " 25.72952   23.154303  28.239485  28.993176  20.793608  27.079714\n",
            " 26.814558  27.052603  30.88869   29.790918  25.643864  28.073341\n",
            " 29.045208  21.692673  19.857122  30.871222  29.872875  32.06897\n",
            " 24.65014   28.585417  28.736301  23.669989  32.090523  23.026234\n",
            " 25.040878  24.63106   21.633911  29.175728  29.126268  22.37236\n",
            " 23.98551   26.367733  30.503128  26.362364  26.209047  21.33356\n",
            " 28.372593  26.783148  25.864431  29.435394  30.242737  30.055187\n",
            " 25.208311  20.638407  23.338432  28.620668  33.230984  31.895418\n",
            " 22.006039  24.826815  15.298397  17.3518    30.311184  27.377466\n",
            " 23.531145  22.718094  27.646671  26.697557  23.104485  23.208809\n",
            " 25.60634   21.99091  ]\n",
            "Layer: backbone.extra.0.1, Channel Importance: [28.796196 13.649392 23.651258 26.345577 31.144297 27.186668 33.909264\n",
            " 24.798786 24.280346 33.155678 20.639925 26.57905  26.707792 27.804972\n",
            " 30.41137  38.40909  26.586973 21.735542 18.406511 24.188898 32.23421\n",
            " 27.110077 24.019691 36.534977 24.040749 27.055038 25.724312 29.834877\n",
            " 27.475792 28.728914 32.649727 25.89958  26.8099   28.628792 23.991558\n",
            " 30.1988   21.408335 39.37616  32.32884  26.974583 23.151457 31.549742\n",
            " 30.167997 28.739508 26.118063 29.64483  30.744646 33.943035 25.345512\n",
            " 30.378288 37.40235  25.30369  29.661848 25.064362 31.53117  29.271431\n",
            " 33.249874 32.90168  20.752653 32.058884 23.695238 28.567255 32.36358\n",
            " 27.778662 31.82124  30.847218 26.052872 25.859207 31.969744 34.85291\n",
            " 28.30581  26.294596 23.593346 26.308672 19.572056 29.91104  25.65933\n",
            " 33.59125  24.492575 24.788843 26.73201  23.968798 26.048252 25.74593\n",
            " 28.444817 25.666204 28.448082 23.010244 37.762478 31.713768 27.348032\n",
            " 19.122799 29.915384 29.399683 27.472517 23.910803 27.128874 26.688086\n",
            " 32.35776  25.612303 31.56812  22.967508 35.255142 31.864939 34.293633\n",
            " 41.62398  28.35667  27.500494 34.543903 28.727938 31.249573 29.699955\n",
            " 31.33465  24.987331 33.49187  26.353754 26.239868 23.444956 34.272255\n",
            " 25.768501 27.495369 30.503036 33.57445  21.895935 31.498077 27.231335\n",
            " 27.528606 23.40484  27.670256 31.68687  23.742353 27.644388 29.046457\n",
            " 29.76445  25.627323 29.504406 19.3618   28.628935 27.584103 28.15488\n",
            " 31.280066 28.827396 28.375074 22.50984  32.112328 30.39734  22.650578\n",
            " 24.390022 29.753477 27.755823 24.574034 30.70119  29.052668 25.57387\n",
            " 30.333242 28.818312 29.009062 34.02582  28.712221 40.080254 22.878359\n",
            " 28.871908 33.910618 23.299978 28.31682  29.184792 19.047035 35.09608\n",
            " 27.866312 27.89671  33.792397 24.892017 32.950356 26.257608 26.09498\n",
            " 32.072536 24.764174 29.540691 31.05009  20.198523 30.87845  24.86734\n",
            " 23.42176  34.09883  28.134691 29.544024 31.11319  20.829437 25.160141\n",
            " 32.730946 24.194582 30.082458 29.25812  27.259487 28.010077 22.140757\n",
            " 26.181974 23.047497 22.954456 32.27128  21.932934 18.701172 27.59149\n",
            " 21.819483 31.023443 21.775072 26.706085 28.61621  27.931398 38.175743\n",
            " 22.030382 32.18225  20.36987  29.882841 29.782288 33.307472 32.039413\n",
            " 22.287498 28.327845 30.454475 20.522318 21.808079 26.33099  27.592909\n",
            " 28.206484 22.263418 30.442957 27.474264 22.427969 27.269451 30.654093\n",
            " 34.513256 27.615505 19.307545 29.574179 24.604963 28.96476  26.150349\n",
            " 30.808565 26.88436  28.169935 33.100464 27.077667 30.624481 30.69894\n",
            " 30.269646 24.803684 19.044218 29.662977 27.964346 26.792065 26.62846\n",
            " 26.11615  27.698046 30.459543 24.63076  22.75007  34.09278  30.086119\n",
            " 26.68917  21.646753 27.795536 28.94805  24.474463 27.37758  22.435944\n",
            " 31.961327 21.23408  28.65946  25.966135 23.847668 22.574266 31.77896\n",
            " 25.566559 22.238214 26.852669 26.937923 29.840658 25.343794 30.23897\n",
            " 23.95345  27.219898 25.603739 26.63457  25.685616 28.28812  34.927586\n",
            " 30.668339 22.23905  32.36348  26.585531 30.494078 27.39754  24.117376\n",
            " 29.71965  27.740046 26.786762 35.203197 24.85941  32.00505  26.054775\n",
            " 36.025455 26.214045 32.934906 27.182055 26.360603 38.127903 26.173603\n",
            " 23.394798 28.412685 24.325264 26.096369 25.57803  26.592466 27.401527\n",
            " 31.721746 24.7818   29.691187 25.562841 25.9073   25.33084  30.351128\n",
            " 30.209677 24.61685  35.25459  26.825155 32.49123  28.781347 28.824467\n",
            " 32.21243  26.427336 31.019894 29.066053 28.670568 25.699911 30.173754\n",
            " 30.943863 28.881313 34.29209  21.40683  30.286423 28.022493 31.11286\n",
            " 21.976711 30.709227 28.534525 22.601643 20.165836 27.126402 26.442455\n",
            " 24.970415 25.816307 22.47801  21.74407  27.690893 33.23818  29.06942\n",
            " 30.800383 24.040518 26.796211 26.049526 28.83799  24.752142 29.827944\n",
            " 25.993412 27.229233 24.827312 28.205349 32.55309  25.523077 39.04135\n",
            " 29.413511 22.708351 27.860584 27.954966 30.444801 27.49893  25.187004\n",
            " 28.66784  25.196642 29.000149 23.040365 25.824419 21.50681  25.596523\n",
            " 27.025251 28.582338 29.857523 21.873392 31.258762 26.943377 26.590338\n",
            " 29.96697  22.482311 27.323248 25.599127 22.433193 27.626713 26.031677\n",
            " 37.36903  28.229515 32.39593  22.112669 35.03845  31.501703 30.057285\n",
            " 27.448416 29.125757 22.877794 32.336246 23.844582 31.527214 28.101942\n",
            " 29.549006 32.305206 24.459654 18.255133 25.38809  27.812725 31.54519\n",
            " 25.729778 29.642408 26.124126 26.474821 26.908195 26.618362 23.457241\n",
            " 26.866999 36.283157 33.436203 35.986904 24.168545 18.5333   34.94889\n",
            " 24.473198 24.986692 23.997736 25.81322  24.511198 23.54318  26.530304\n",
            " 25.827524 28.378174 29.014204 27.257416 24.29613  26.35191  30.501123\n",
            " 27.303698 22.208317 30.7989   28.448488 30.96246  19.142708 29.252428\n",
            " 37.43688  25.527437 26.58237  22.18282  24.05669  24.445524 26.79682\n",
            " 30.218554 31.998972 23.042881 22.17245  25.4191   25.362469 22.456326\n",
            " 23.86854  28.350508 24.925331 32.90499  30.7072   24.051647 24.852642\n",
            " 31.183374 27.683603 37.62313  28.534204 30.052748 34.73019  23.704775\n",
            " 30.911861 25.91647  23.822071 27.163315 22.424608 23.636806 31.846087\n",
            " 30.29194  27.40118  27.237843 32.94991  24.672155 26.849562 24.865314\n",
            " 22.880182 30.79777  24.484287 31.596914 29.363403 28.71478  20.640707\n",
            " 29.231443 24.798176 31.136663 26.485832 27.928219 26.23956  28.137781\n",
            " 19.625185]\n",
            "Layer: backbone.extra.0.3, Channel Importance: [24.274183 34.445667 27.193531 19.710892 24.593084 25.3939   21.491104\n",
            " 36.728134 22.489439 26.031837 26.430834 25.549305 27.81514  19.42212\n",
            " 20.704668 26.22768  30.945356 37.765827 30.582863 27.40081  20.701574\n",
            " 21.397991 24.409546 33.979908 18.424185 22.562647 23.175991 31.810545\n",
            " 24.285236 23.950079 28.650808 23.042625 23.455881 26.5236   27.148298\n",
            " 19.938547 24.447248 29.468563 19.45676  28.9191   22.467733 15.331433\n",
            " 25.181492 27.44395  30.790686 23.802341 29.05376  26.513039 23.366596\n",
            " 21.615099 20.662514 22.570993 28.869265 22.638645 24.887756 29.910269\n",
            " 26.467619 25.244625 23.10277  26.40874  33.311424 25.265133 17.037674\n",
            " 30.149393 28.034962 28.12956  33.008026 21.408773 28.727098 26.309538\n",
            " 28.041582 26.31941  29.44746  19.476376 26.440973 25.460344 30.98666\n",
            " 36.119488 25.034222 34.188522 18.147137 29.893465 15.813094 28.070679\n",
            " 31.273067 24.843596 25.7476   23.330826 22.874584 25.842693 27.129942\n",
            " 21.306042 24.450407 30.101095 28.550879 27.897379 32.980694 24.485418\n",
            " 30.693039 32.095646 23.909048 18.911882 29.050884 36.85481  25.509703\n",
            " 27.538944 29.225979 25.095655 35.758804 26.436697 29.85749  23.332308\n",
            " 22.354168 25.302507 21.275486 20.882788 19.923613 19.629248 25.34575\n",
            " 24.094385 28.624619 18.592075 21.80488  22.4758   18.891909 26.244219\n",
            " 28.788631 26.282116 23.936296 23.193182 25.294119 27.868711 24.241816\n",
            " 22.532562 26.401848 32.320972 23.071987 22.051386 29.627594 29.060501\n",
            " 30.922388 29.777973 33.196297 27.853151 25.329018 32.048584 26.15328\n",
            " 26.163246 21.981564 21.913847 25.826403 31.832079 22.034218 20.40515\n",
            " 23.200985 28.542988 32.086624 22.547745 31.23018  30.466522 27.255703\n",
            " 29.200567 34.34126  20.921265 43.31085  26.450455 20.412554 22.29992\n",
            " 34.010036 34.01372  18.156061 22.296026 24.533585 23.188234 23.036713\n",
            " 24.740442 24.513973 24.244091 23.308132 28.49428  26.617298 21.96433\n",
            " 20.712816 21.007504 27.196686 27.706339 35.099762 23.080103 22.531124\n",
            " 26.010532 24.810575 19.98976  24.904444 25.502922 30.762157 32.416668\n",
            " 24.20884  26.990965 25.729004 25.622154 26.34516  24.776814 28.532326\n",
            " 18.98958  37.17594  20.136106 27.647158 29.188549 25.743711 22.250418\n",
            " 32.613358 20.467514 22.788187 22.346746 17.195656 24.892664 19.960503\n",
            " 22.902735 23.684269 23.556303 23.783228 23.439354 22.348728 23.290031\n",
            " 27.74369  25.820576 28.495848 24.92352  23.15313  24.621376 29.45859\n",
            " 29.307383 20.746908 31.444809 28.277596 34.379227 21.788948 33.82785\n",
            " 23.983652 23.604853 25.890894 25.960009 20.434284 23.328531 24.843302\n",
            " 20.449215 25.890875 26.92546  20.289816 31.360395 33.878376 28.742044\n",
            " 24.938635 22.054682 27.365044 20.304909 22.636456 23.16648  23.266953\n",
            " 37.80973  33.51381  18.800793 26.459478 23.3556   24.02946  26.646023\n",
            " 22.605663 28.394651 26.972078 30.086353 21.853895 29.14126  22.985312\n",
            " 23.62208  28.535881 24.158226 32.590416 33.25939  26.006227 24.085869\n",
            " 25.0723   27.51291  25.787136 23.203697 25.641844 18.277206 29.568638\n",
            " 22.946745 20.74885  32.113792 32.7339   22.200327 24.302666 25.908913\n",
            " 27.627157 24.561974 18.911966 23.990923 24.719578 30.630728 26.973091\n",
            " 28.89857  23.925945 22.719263 22.22415  21.904121 17.642838 31.832718\n",
            " 26.062538 24.067316 21.442478 19.493553 18.405874 23.851284 20.6994\n",
            " 23.75851  35.313614 26.186083 17.912615 27.795855 22.439848 21.47188\n",
            " 14.756695 24.961416 24.565437 23.930311 23.388334 26.00908  23.355011\n",
            " 27.931992 21.69166  22.198406 23.306955 19.209015 28.81838  26.163767\n",
            " 32.07066  20.899252 22.697386 23.76662  25.84246  30.210573 21.903683\n",
            " 29.264593 27.933306 22.617706 26.847336 21.692642 25.887465 36.59722\n",
            " 24.732773 21.195744 21.70878  28.520102 25.758978 30.156025 22.003876\n",
            " 24.828608 23.250338 26.345106 28.846708 34.296474 21.284489 24.86989\n",
            " 24.01505  29.10313  26.366844 28.726753 25.61968  24.606354 26.278385\n",
            " 20.93673  22.750628 26.259672 24.793446 28.822557 21.013603 26.244236\n",
            " 33.376816 22.65494  27.146168 23.345594 24.674776 24.252178 22.790066\n",
            " 27.145216 24.366093 26.378155 30.444876 20.367567 20.878283 24.40602\n",
            " 34.189888 24.062696 24.878792 29.15509  22.64586  25.28562  23.278149\n",
            " 27.883852 23.10351  24.552782 27.29641  23.398739 31.754713 24.881472\n",
            " 18.094973 25.338112 22.71815  25.227392 24.615828 20.786173 21.291258\n",
            " 18.955086 28.82842  21.275236 32.554676 18.891022 20.79163  27.628769\n",
            " 25.94966  31.635275 28.286163 25.721766 25.401123 26.512053 27.029879\n",
            " 26.526451 31.585268 25.73379  21.572645 27.516636 25.649885 39.706684\n",
            " 25.531643 34.60575  22.486784 21.504538 25.465078 25.321833 28.296844\n",
            " 19.80155  25.213667 28.584639 20.119907 25.197174 26.558073 28.87158\n",
            " 27.26469  23.008884 29.97814  20.009804 21.014854 19.924152 18.644892\n",
            " 21.394913 25.831537 26.12918  22.955162 23.50135  37.356724 23.768991\n",
            " 19.651207 29.772093 23.154943 26.44175  32.69579  25.727417 24.872528\n",
            " 25.905783 25.110153 39.395462 31.366446 22.69158  33.418957 22.920061\n",
            " 28.292196 35.878677 26.169556 15.380438 26.820648 35.48218  27.819271\n",
            " 20.419802 19.52382  30.801046 27.618961 25.601334 21.861584 27.292473\n",
            " 28.391436 30.603945 19.778982 29.046223 40.946766 20.64462  25.201908\n",
            " 24.071154 21.699863 28.353388 26.221998 31.037998 24.261745 33.973755\n",
            " 21.839928 23.537498 21.499517 30.53637  21.91659  22.523586 25.410824\n",
            " 23.572649]\n",
            "Layer: backbone.extra.0.5, Channel Importance: [17.333506  10.319733  31.535252  13.691687  24.359823  28.821732\n",
            " 29.249535  10.915227  17.789627  29.897419  20.459908  17.31798\n",
            " 19.643776  21.31277   14.177246  26.034115  24.393171  27.416729\n",
            " 17.600664  25.144997  21.835962  17.985987  24.046505  21.489988\n",
            " 19.606493  25.800066  22.968573  22.003569  30.490936  26.874388\n",
            " 26.055166  23.831648  19.588602  15.874329  28.329592  16.586533\n",
            " 23.38556   13.950682  17.145525  16.92855   26.375616  18.844933\n",
            " 30.981133   6.227383  18.59276   31.944286  24.503231  26.123325\n",
            " 28.553503  19.479984  23.659893  25.103964  29.186941  20.01598\n",
            " 16.66226   23.500483  16.199518  20.291775  25.164629  21.739637\n",
            " 14.590071  14.70174   18.139023  23.106064  22.109005  30.311007\n",
            " 30.329342  23.817396  24.114574  24.2205    23.770449  23.0101\n",
            " 22.184958   8.349048  10.378133  22.810726  20.851145  25.427895\n",
            " 21.255064  26.411777  26.650072  23.775267  26.163363  27.62759\n",
            " 21.226974  27.678679  20.691322  18.424095  28.261692  27.725916\n",
            " 22.444723  22.982244  23.679623  32.381084  20.417004  28.219358\n",
            " 34.860825  23.162096  17.6595    18.919537  23.935453  20.59444\n",
            " 22.136415  22.03696   31.112394  21.777761  18.47698   20.781746\n",
            " 29.048357  10.593926  25.022697  13.110885  27.423847  21.850365\n",
            " 23.85259   16.259754  24.014412  19.6344    33.587185  36.004505\n",
            " 15.781523  29.107664  27.332783  23.776373  19.39184   19.503866\n",
            "  9.163848  28.825222  12.946115  23.795889  25.36909   29.891575\n",
            " 14.609233  23.856136  23.007498  24.233238  27.801077  22.210814\n",
            " 26.384464  22.808287  15.368833  18.515541  22.801552  35.426926\n",
            " 22.507261  26.723322  38.151356  29.270735  22.487083  25.704554\n",
            " 29.494406  17.566141  18.643122  21.09922   26.645586  10.0463\n",
            " 13.427742  21.829329  26.914776  31.595352  23.426273  20.920773\n",
            " 30.768608  19.216413  19.086084  25.368454  24.597942  14.0721\n",
            " 28.077894  20.582066  27.800892  17.653534  25.853334  17.698315\n",
            " 29.211485  25.419035  18.34804   23.657394  28.427816  34.05112\n",
            "  8.636812  17.861134  30.758383  26.516006  25.815697  17.960697\n",
            " 22.030573  24.300072  19.173817  28.881184  18.160778  22.970987\n",
            " 24.312738  32.6349    27.409063  21.106958  19.478064  28.628668\n",
            " 24.475039  15.261394  18.770573  17.565727  27.330868  20.802664\n",
            " 18.759027  22.857264  23.828102  20.392996  29.228432  26.780096\n",
            " 23.7606    23.246479  13.771547  20.671358  28.72328   29.344099\n",
            " 11.934375  17.362793  24.722908  32.980785  25.333277  10.457739\n",
            " 21.891901  28.568747  23.106443  28.071283  26.350353  27.572191\n",
            " 23.561695  20.09119   24.676546  18.445978  20.5456    27.736292\n",
            " 22.833767  20.015697  31.567753  20.803518  17.394022  27.985186\n",
            " 22.285522  18.252247  29.816713  30.393904  20.898335  32.997116\n",
            " 13.024378  17.474495  30.695688  15.278978  27.861446  29.381687\n",
            " 19.314068  16.723602  19.298609  31.275826  15.36332   22.693104\n",
            "  9.985155  19.157894  23.477606  28.967121  14.63206   34.22213\n",
            " 27.848682  15.429954  31.627659  22.379498  22.847954  25.586575\n",
            " 27.586697  24.467196  28.931368  14.377992  20.126175  24.239567\n",
            " 23.450403  24.146553  29.652752  23.882147  23.479717  22.339329\n",
            " 33.50453   14.037044  24.700443  17.607119  22.32236   21.218954\n",
            " 20.207605  19.112865  19.02227   21.555965  17.677584  17.4968\n",
            " 22.010996  14.7414    17.8811    27.083654  24.997221  20.596695\n",
            " 17.891476  12.891709  25.129753  21.290812  13.153404  12.844335\n",
            " 23.029926  29.570206  18.058952  25.335415  24.352106  10.20846\n",
            " 24.764212  21.344818  24.770529  27.591467  23.767466  31.617926\n",
            " 23.776814  21.06813   32.017776  17.44896   15.194908  25.900301\n",
            " 17.895153  28.038681  26.793842  10.488579  25.792206  15.969903\n",
            " 19.998999  17.84223   18.070465   6.510553  23.682884  13.185987\n",
            " 16.298412  19.60803   25.16052   22.19729   32.751595  22.276155\n",
            " 22.34366   33.97131   22.817812  20.785849  21.607996  30.287075\n",
            " 25.394268  15.557552  18.224525  24.45229   27.984982  19.054123\n",
            " 17.783857  18.033222  27.66454   27.050602  14.678265  28.402845\n",
            "  8.658046  24.425606  33.08661   21.449884   6.631277  19.75264\n",
            " 23.879221  36.417927  27.219166  16.57982   23.43548   29.07685\n",
            " 26.345284  28.081108  28.510801  11.326784   5.0938473 20.181858\n",
            " 27.791811  13.208492  24.721638  21.518196  25.054272  14.858693\n",
            " 19.297417  23.363676  15.883122  26.776785  26.79845   20.435537\n",
            " 20.624468  12.966091  24.326878  16.57787   26.305197  15.118791\n",
            " 25.214512  20.001392  28.27679   26.057556  12.608565  28.749159\n",
            " 41.616684  22.374979  15.11655   18.625772  27.79105   24.348095\n",
            " 22.175846  19.494165  15.76821   24.347702  19.434149  21.54553\n",
            " 21.542074  21.96418   22.158863  22.323874  23.336073  24.578922\n",
            " 23.011677  28.971695  24.355581  23.08964   22.119833  23.86251\n",
            " 23.718683  20.078644  23.513386  24.055458  22.823029  19.491732\n",
            " 17.371893  16.710018  22.412329  21.28324   17.504707  34.78271\n",
            " 38.03534   15.467012  25.811266  26.51355   12.697436   4.8722005\n",
            " 19.861654  34.924423  13.152769  27.85648   20.747393  24.700092\n",
            " 24.035418  24.1147    25.853363  13.467318  19.445576  20.586994\n",
            " 18.662045  15.405352  27.671616  30.524328  21.917385  21.924007\n",
            " 16.881191  27.250738  34.732536  23.02623   17.427406  25.630295\n",
            " 29.209904  28.178162  19.995716  25.547314  19.329163  15.894607\n",
            " 23.883972  27.099035  20.138964  17.401754  19.711979  26.408785\n",
            " 17.69457   13.794042  18.957527  19.197674  14.645097  15.154878\n",
            " 17.571178  19.92732   23.506783  22.452423  15.256052  32.20677\n",
            " 23.356966  20.452005  24.384634  22.01601   23.895418  13.343499\n",
            " 28.673746  27.578617  14.369172  28.938147  20.637842  24.312353\n",
            " 23.859428  22.345024  34.0571    16.611423  20.035461  23.744858\n",
            " 36.53426    5.0419917]\n",
            "Layer: backbone.extra.0.7.1, Channel Importance: [19.364042 18.588257 19.445202 ... 10.758657 21.943329 11.932286]\n",
            "Layer: backbone.extra.0.7.3, Channel Importance: [8.0399685 9.295662  6.830643  ... 7.1139035 9.964346  8.688981 ]\n",
            "Layer: backbone.extra.1.0, Channel Importance: [ 3.9192147  8.476926  12.071821   9.122967   3.8787644  8.621489\n",
            "  9.808471   7.126477  10.414671   7.7426643  8.778092   4.1895685\n",
            "  5.5549474  6.2636733  9.996828   9.06866    6.4735565 11.269875\n",
            " 11.032832   9.767112  10.635174  11.279926   5.9761987 10.053563\n",
            "  9.611414   9.030887  11.669853  10.765963  11.013233   8.422364\n",
            " 11.930675   9.672767   9.645099   6.5323014 10.381326   2.386458\n",
            " 12.266908   8.94569    4.8075924 11.119203   9.937367   5.2731943\n",
            "  7.037406   4.4293094  8.798799   9.95058    9.410016  10.538972\n",
            "  8.117019   8.765034   7.785727   8.731727  11.052777  10.527282\n",
            " 11.660267  10.743796   7.2147946  8.988986   7.303769  13.332245\n",
            " 10.323366  11.0698805  9.847824   9.485252   7.9270005  6.978561\n",
            "  5.098386   7.360544   2.6580353 11.351261  10.130758   9.959742\n",
            " 11.146447   8.624523  11.11798   17.361214  10.084575   5.428338\n",
            " 11.477615  10.444239   8.460465   6.2803197  1.6699511  7.0990596\n",
            "  9.417997   7.7211804 10.710488   9.583241   8.226155  10.009937\n",
            "  8.36593    8.11704    6.3779397  8.628456   6.628807   7.358124\n",
            " 10.489596  10.729061   8.2148905 10.090688  10.908793   8.541588\n",
            " 11.700562   9.557531   6.118867   8.954172   3.367458   9.793345\n",
            " 11.245511  14.915176   8.424041   9.324285  11.965221   8.964141\n",
            "  8.341375   6.9203753  9.411111   8.774893  10.241548  11.17745\n",
            " 12.160964   8.509398   9.950702  11.541283  11.813025  14.069203\n",
            "  4.954229   9.535232   8.821171  12.851788   8.821629   1.7640016\n",
            "  7.7570834  8.239251   8.634375  10.36828   10.834936   9.861914\n",
            "  1.7076974 16.78733    6.986151   4.1134186 12.027853  13.346424\n",
            "  6.8553305 10.556187  11.526312   9.132186  10.569169   9.762597\n",
            "  7.8632617  8.884549  12.444207   6.110431   9.8847475  7.8190665\n",
            "  6.7956247  6.079275   8.5294695 12.670562   5.8762584  7.5297084\n",
            " 14.275966  11.211374   9.532176   8.3492365  9.400935   8.882572\n",
            " 11.2463045  8.2259655  8.992306  11.421853  12.64804   10.217466\n",
            "  9.511058  17.245655   6.637162   7.266251   9.382351   9.7383995\n",
            " 11.562944  11.055265  11.376222   7.874657   8.47952    7.803755\n",
            "  7.457088   8.84789    7.992035   8.7575445 10.056834   9.91789\n",
            "  4.0734463  8.770243  10.014034  12.682669  11.211685  11.520593\n",
            "  6.591089  11.028328   9.562585   9.519184  12.198396   9.882517\n",
            " 13.928394   8.62186    6.8302226  2.456277  13.526343   6.9128613\n",
            " 12.48468    8.433721   3.1100678  9.252803   7.806717   3.7069263\n",
            "  8.520222   7.6843987 11.507673  11.486347  10.816116   6.499039\n",
            "  6.4483314  2.945621   5.7368903  9.826792   3.582958  10.28505\n",
            "  5.1779723 10.063462   4.052388   9.26609   10.317381  10.229056\n",
            "  9.117134   9.440354  11.740709   5.3889647  6.109739   5.753439\n",
            "  9.45453   10.483283  12.056406  12.109082   5.7343535  8.607705\n",
            "  8.28523   11.318806  11.462146  11.089794   8.927125  10.028499\n",
            "  8.681203  15.310946  10.687389   8.444551 ]\n",
            "Layer: backbone.extra.1.2, Channel Importance: [ 9.877122   7.425787  11.067047   9.37105   14.6776085  6.691683\n",
            " 10.29634   14.86232    3.8937542 14.802222  12.681606  11.749995\n",
            " 10.1331    12.965521  13.158198  14.610884  14.240532  13.949387\n",
            " 13.736796   5.9634175 15.301284   5.601582   3.7567227 11.446675\n",
            "  9.18231   14.569957  11.601259   6.6252213 18.977251   7.804844\n",
            " 14.04859    8.782543  10.306508   4.7050257 14.655011  15.880284\n",
            " 10.58411   11.661871  14.10838   11.572718  12.354075  12.8861685\n",
            " 12.67991   13.994941  10.9697275  1.629456  11.332648   7.660295\n",
            " 13.84883   10.078397  15.261163  10.984123  14.394826   9.593506\n",
            " 10.806882  12.646727  15.899008  13.830079  11.302547  12.508594\n",
            " 11.636574  18.034536  10.483122   2.265887   8.846193  10.663326\n",
            "  8.496278  13.573837  11.735426   1.4872664 14.425201   9.050478\n",
            "  8.512565  15.28977    3.2262993 12.215046  12.036021   1.5539424\n",
            " 11.802829   8.312619  14.835708   6.3245907  8.724091  10.291093\n",
            " 11.138172  11.698845  12.025031   2.3192468 12.997     11.953885\n",
            "  8.771133   8.713424   7.3601737 11.088977  13.748896  15.406401\n",
            " 14.218391  11.530981  11.861768  16.040642  11.819901  12.3736\n",
            " 14.556907  13.821112  12.76469   16.267677  11.027253  13.781983\n",
            " 11.980619  11.42995   12.207218   8.296799  12.204633  10.049578\n",
            " 15.578479   1.5706594 10.437471  13.198331  14.75239    9.981453\n",
            "  1.5681235 11.784144  12.131904  13.595605   5.3319917 13.254495\n",
            "  1.5126917 10.215822   8.223634  12.07241    6.1669097  8.700848\n",
            " 14.250469  12.267049   1.4581535  7.856473   9.260159   6.535455\n",
            "  9.690895  12.6402    11.795887   7.8088093 13.840224  11.144963\n",
            " 12.678025  11.830735  11.245066  17.21967   11.869312  10.671841\n",
            " 12.442509  12.975241  13.411484  14.157965  14.899956  10.384776\n",
            " 11.530257  10.52947    8.164372  11.982954   7.7572503 10.163977\n",
            " 10.370617   8.22814   12.5792265  1.5304283 15.733078  13.124057\n",
            " 12.042592  10.503966  13.305315  11.513357   4.809835  11.79696\n",
            " 13.36815    9.746079  12.040466  11.55653   12.079393   3.3603296\n",
            "  9.27686   13.729014   3.447782  13.851341   9.845847  13.318186\n",
            " 16.087324  14.147729   3.6666632 10.3400955 10.408859   5.545807\n",
            "  8.911916  10.48124    6.51049   10.469605   8.891253  11.439247\n",
            "  8.463135  11.293641  11.613027  12.181449  10.43593   10.043202\n",
            " 15.60746    8.8300705  7.735899  11.635599  12.6045265  6.9395847\n",
            " 13.170921  11.627642  11.511608  10.784631   9.964451  12.702618\n",
            " 11.270904  12.332368  11.016336  13.492147   9.163431  10.617803\n",
            " 10.94155   14.666092  12.01007   12.103647  10.186934  11.064719\n",
            "  2.2314248  8.096459  13.341438  12.062558   9.123796  10.168426\n",
            " 10.22847   13.920478  11.431759  10.306398  11.354748  11.134577\n",
            " 14.490057  14.287945  14.810386  11.186947  10.347054  11.2128\n",
            " 10.779394  14.900078  10.160265  14.68203   11.057694  11.191794\n",
            "  9.77844   11.787743  14.476545  12.44228   13.610718  11.698161\n",
            "  9.759042  13.609011   2.73743    8.628019  17.096386  11.279995\n",
            "  8.0778675  7.354538   4.276042  19.341759   9.506822   2.845944\n",
            " 12.487845  15.048499  12.742817  11.944966  10.8410425  9.128576\n",
            "  6.9798493  6.055108  16.668287  10.613364  14.020073   1.5073358\n",
            " 10.828395   9.2851515 14.857024   9.962397  11.7733    10.612128\n",
            " 12.301088   3.6623955  8.280897  10.693675  11.819784   9.262011\n",
            " 10.406136  16.214779  14.674796  12.952517  12.022916  12.497469\n",
            " 12.595898  10.42638   10.420401   5.91241    2.6881695 12.546846\n",
            " 13.355935  10.460924  10.298167  13.208212  11.378471  13.946803\n",
            " 12.32978   13.56152   10.198182  13.333276  12.398454  10.730814\n",
            "  4.760504  12.279064  15.148367  10.4933195  7.7976108 11.278947\n",
            "  7.2771544 14.280681  11.793265   4.7869806  8.459174   9.528473\n",
            " 11.874204  10.291493  11.204742  13.448332  10.149011  15.928471\n",
            " 12.505291  13.614203   9.469092  16.231281   8.649258  16.966726\n",
            " 10.33075   14.164232   6.57479   12.099959  12.057331   5.244208\n",
            " 13.474017  13.81172    5.5403337 12.51421   10.419071  13.525659\n",
            "  5.462134  11.546363   1.989205  10.36498   10.944815   9.219549\n",
            " 10.393279  12.354147  11.55149   14.203104   8.433649  10.15583\n",
            " 15.550406   3.3375869 10.807758  12.069688  10.711973   7.6696835\n",
            " 12.801022   8.264086  12.047844   2.4324114  1.5170803  9.327187\n",
            " 10.833016   9.494898  14.272506  13.413408  11.598025   9.422035\n",
            "  6.4349875 13.151336  14.232681  15.294144  12.969582   7.5218573\n",
            " 14.861658  11.262081  13.6536    10.299806  11.901146  13.829089\n",
            "  6.31057    4.091273  13.242107   7.2517157 10.973952  11.78523\n",
            "  3.3211596 11.887788   8.720195  10.132699  12.475176   8.979432\n",
            " 13.944013  15.279022  11.419433   7.231438  10.254516   5.075331\n",
            "  9.510989  10.64432   14.4997635  6.6841273 11.370456  12.381595\n",
            " 11.567496   7.5363026 11.185442  10.079487  14.7718725  9.124564\n",
            " 13.256362  13.390303  12.779863  12.236215   8.241363   9.617836\n",
            " 11.1769905 11.230802  11.046241   4.493282  14.7633    12.038385\n",
            "  6.9277925 14.354497   3.3235996  4.768789  11.886992  11.720466\n",
            " 11.601792  11.867706  10.376119  10.545469  13.7686405  9.887568\n",
            " 13.97186   10.199755  13.110037   8.839653   7.2909374  5.2109003\n",
            "  1.9283341 12.655051  11.6145525 10.584512  13.887557   9.246848\n",
            " 14.980734   4.128362  10.251512   6.741816  11.426305  11.936795\n",
            " 13.306189  13.296754  11.015205  13.218153  13.886248  10.451999\n",
            " 11.568649   5.7419615 12.304288  13.669753  13.6050825 15.030954\n",
            " 14.264171  11.348568  15.122663   7.9488463 10.479372   7.999201\n",
            " 12.562958  13.592029   2.801793  11.730461  16.367407   7.942419\n",
            "  9.847723  13.282067  13.068786  10.975311  14.580445   9.456526\n",
            "  9.280403   1.52453   12.159223   4.9638953 13.260756  12.016293\n",
            "  9.518269  12.701917  12.02636   12.289708  10.58332   14.802737\n",
            " 12.114952   8.764498 ]\n",
            "Layer: backbone.extra.2.0, Channel Importance: [ 6.74666    4.575854   5.7721963  7.7509933  7.6670437  7.1949897\n",
            "  9.585184   1.1057922  8.612363   1.7344922  9.368943   9.868967\n",
            "  7.248751   8.033607   5.16628    4.3947797  7.2140136  8.128277\n",
            "  5.9288125  8.461706   8.782437   7.320851  10.01098    6.0641794\n",
            "  7.9057136  7.233328   5.0295906  7.5009174  8.197817   2.6189666\n",
            "  5.616332   6.27043    2.9103744  6.8155603  7.429794   5.9356565\n",
            "  9.369681   8.001132   2.6005337  9.127284   7.3497744  6.8043365\n",
            "  6.297689   1.1526488  7.659952   8.027999   3.2924871  8.756431\n",
            "  6.1772165  4.0921454  2.1505075  7.384419   5.8992834  6.654601\n",
            "  7.7443094  8.202307   8.830996   6.5370727  3.7172847  7.623337\n",
            "  7.439319   7.0606794  8.238707   4.3067803  7.5143213 10.1570635\n",
            "  6.7387886  1.1206232  6.735951   9.705835   6.4700203  5.5203276\n",
            "  7.482474   7.3571606  7.046834  10.108853   5.588928   3.6529028\n",
            "  2.0614607  7.9590487  8.338192   2.9986515  5.2242103  1.0776784\n",
            "  8.575331   5.9898734  8.077665   1.1105729  5.955622   1.826315\n",
            " 10.039787   8.020736   9.295126   8.043561   4.998824   1.9827712\n",
            "  1.2383955  7.8561544  5.944644   8.369941   6.798497   3.1334949\n",
            "  5.9191914  6.635964   8.471094   8.728109   9.629429   6.53248\n",
            "  7.4714684  7.3980827  6.2524734  7.4106793  8.026028   7.9592714\n",
            "  7.7405033  8.520026   7.083027   7.8702908  4.5849533  8.091904\n",
            "  4.3596787  8.640202   6.6691103  7.3832517  7.422673   6.910039\n",
            "  8.1902075  6.3809643]\n",
            "Layer: backbone.extra.2.2, Channel Importance: [ 8.652637   9.506675   8.844102   9.720013   9.540204   7.3871093\n",
            " 11.4661     7.7720485 10.043586   9.746298   6.9959035  8.403411\n",
            "  7.0424623 10.203372   6.7552557 10.63225    6.9544764  6.724291\n",
            "  7.862014   5.646807   9.524216   9.591952   6.681285   6.300711\n",
            "  1.0931329 11.1294     6.92655    9.79006    8.495055   7.063676\n",
            "  5.9105067  6.1415486  9.040083  11.442842   7.556673   8.130037\n",
            "  8.423279   9.51225    8.1158085  7.741886  10.320868   5.6908865\n",
            "  8.532302   7.0941124  4.902018   9.827252   8.262441   6.938493\n",
            "  1.8087683 10.392642   1.0576974  9.277378   9.898667  10.355342\n",
            " 11.228177   1.039893   6.865759   6.892621   5.9407563  6.2998757\n",
            "  8.574331   9.253336   1.7829473  7.470009   5.9581885  9.394946\n",
            "  7.259533   9.582142   9.935909   8.91786    9.215039   8.679698\n",
            "  9.42703    7.620311  12.1464405 10.863091   3.6762323  1.6760601\n",
            "  8.214572  10.853503   6.483424   8.283403   9.475628   8.184145\n",
            "  8.3548355  9.679225   9.05027   10.164601   7.8402634  5.9048705\n",
            "  8.151781   8.786193   6.6650505  5.2486625  8.986159   5.792519\n",
            "  8.847309  11.048444  10.450385   9.911812  10.077099   9.576023\n",
            " 10.437791   5.4058685  9.378519   6.204056   7.5909677  9.244124\n",
            "  5.138506  10.694735  10.667333   4.082389   7.5469623 11.240089\n",
            "  6.0692267  3.137564   8.031502   9.425684   9.771212   7.902533\n",
            "  1.3541017  7.6356373  7.226664   9.785107   6.032482   8.228988\n",
            "  4.9979043  4.3336606  1.136441  11.99378    1.2077029  5.7295046\n",
            "  4.935113   7.1052046  6.341828   9.691225   9.250335  11.211022\n",
            "  9.336408   9.254002   7.596232   9.836779  11.867573  10.218558\n",
            "  5.7288613  8.948213  10.362756  10.478081   2.3274872  9.329597\n",
            "  9.238693  10.858104  11.1390085  7.4996214  8.259366  10.339067\n",
            " 10.280887   8.295387  10.312343   7.1350417  5.438493   8.244903\n",
            "  1.0669066  7.655095  13.33493   10.58323    9.953228   5.351881\n",
            "  9.158272   7.667091   5.14197    8.234867   7.7473273  6.658191\n",
            "  5.727266   9.68755    1.067981   8.441796   9.402676   2.9705346\n",
            " 11.165191   6.749571   9.592815   4.1281734  8.980699   9.442868\n",
            "  8.431927   8.116614   8.1989975  7.8723636  7.464717   8.843336\n",
            " 10.000747  12.474592   3.630455   9.554584   7.8108754 10.119969\n",
            "  5.2470965 11.824069   8.187888  10.610747   8.652676  10.22484\n",
            "  8.404744   7.327958   1.075121   8.394811  10.643032   8.000596\n",
            "  5.521701  10.536579  10.137935  11.829416   9.043086  10.597419\n",
            " 10.022125   1.0450375  1.0555333  2.3349776  9.345824  10.3094425\n",
            "  9.511652   9.664027  11.495278   7.574908   9.587574   4.303285\n",
            "  9.574561   5.4916205  8.278605   8.36796    1.0965542  6.741766\n",
            " 10.16833    9.711255   9.135047   8.550222   9.087897   7.956002\n",
            "  5.2208257 11.510722   9.489109   9.12672    8.883461   9.347275\n",
            " 10.643229  10.819606   9.969745   6.568665   7.4593735 10.756373\n",
            "  7.154414   7.6302867 10.449686   6.4748673]\n",
            "Layer: backbone.extra.3.0, Channel Importance: [4.906504  2.181432  4.699705  5.0022516 4.8358755 4.771333  5.5497613\n",
            " 1.4939208 3.276216  4.6741304 3.2880328 3.198387  4.2490473 2.3082547\n",
            " 4.0574365 4.236278  0.8816066 2.3184032 0.9059866 1.7214239 6.0362463\n",
            " 0.9553567 4.899772  4.6088614 1.5588735 2.0881104 5.2577095 3.8628066\n",
            " 4.6372647 3.9438553 3.8398929 3.6251004 4.8020477 5.1525235 4.0993423\n",
            " 2.3423204 5.0708075 4.581137  3.5474167 1.4960362 4.462486  4.618493\n",
            " 3.3717818 3.5849216 3.5211737 2.3782768 4.0780325 3.7218118 3.6194916\n",
            " 1.8148475 5.590121  3.047555  3.2560496 4.903988  5.44315   1.7465762\n",
            " 4.733029  3.0862293 4.66328   5.6740317 4.07362   4.1098504 1.0294142\n",
            " 4.4598923 3.9212065 1.6452243 6.2371836 3.1986716 5.231241  2.861521\n",
            " 1.6905972 4.541277  3.5723045 3.6721468 1.9595752 2.7062008 3.3082929\n",
            " 2.1519842 4.076556  3.791527  5.5739837 3.6901658 5.025033  1.8735348\n",
            " 3.9326446 3.6105454 4.955147  4.5905123 4.552728  0.8426099 5.046602\n",
            " 2.7007804 5.9387417 3.481712  3.9241335 5.012196  4.3787146 3.557849\n",
            " 4.855332  0.7525108 2.794873  0.819999  5.1446357 3.5401008 5.408692\n",
            " 4.260468  4.60193   4.863065  2.791412  1.323075  3.09675   4.1729794\n",
            " 4.426424  4.4662213 4.176877  3.0285194 4.065033  3.2262774 4.084833\n",
            " 1.1346166 5.006411  4.729289  4.9328403 2.1860838 3.430085  1.0311066\n",
            " 0.819566  5.3656125]\n",
            "Layer: backbone.extra.3.2, Channel Importance: [ 5.2702293  6.1480556  8.662679   7.2208986  8.390242   6.852393\n",
            "  5.892565   4.3373446  6.910893   8.288295   8.812211   7.8069496\n",
            "  7.4614267  1.0710214  6.5076885  8.400941   7.9639635  7.155818\n",
            "  8.920594   6.5916944  9.131004  10.706917   6.3792706  6.6511703\n",
            "  8.0573435  6.1687536  8.415779   6.2364326  7.2335863  6.416544\n",
            "  2.107329   7.7925982  6.788145   5.2718005  6.1635904  1.0631194\n",
            "  6.1824217  8.296474   8.703316   6.1772966  2.3517497  3.2583294\n",
            "  7.3942523  3.6521893  6.0279827  1.0994638  7.060429   1.4567533\n",
            "  9.227434   6.9455605  7.5778756  1.0718858  2.3775995  6.9347563\n",
            "  1.2087226  8.8382435  7.8045726  2.509298   7.6813684  8.725461\n",
            "  7.048589   6.587024   8.060475   9.594428   8.086823   7.0761623\n",
            "  8.777236   7.0415163  1.1674292  1.0759517  8.414102   1.2343602\n",
            "  7.0785007  6.979857   1.3773341  8.787436   3.6439548  6.8702393\n",
            "  8.7898445  9.015863   7.329      6.596804   5.131518   5.8152146\n",
            "  5.105304   7.6136856  7.3186326  5.743335   5.4098043  7.2020063\n",
            "  1.0764742  9.12565    5.420093   7.0462337  3.7405703  4.3719234\n",
            "  7.158618   3.6781132  6.3221326  7.620899   9.936412   6.8383684\n",
            "  1.1022636  1.5744916  2.1720293  5.58702    5.8000174  5.2704167\n",
            "  7.333409   4.4367967  6.4371977  5.400712   7.1927423  7.7501373\n",
            "  6.6439176  8.6156025  1.1222388  6.8309245  1.0866297 10.017125\n",
            "  6.3207135  7.420195   6.225451   8.054456   5.781064   8.56919\n",
            "  8.809278   6.357216   8.356856   7.9177055  7.4454274  4.5243\n",
            "  8.994678   5.999377   4.7877827  6.8871007  6.665053   5.2329597\n",
            "  7.891144   1.084983   7.4841976  5.86005    2.0116646  9.105789\n",
            "  7.4066114  8.920429   6.2332816  7.505341   7.151687   8.440002\n",
            "  7.7349324  5.66139    6.09519    2.3030803  7.8795915  6.8950763\n",
            "  9.651815   2.0922623  9.027738   5.0762925  1.0717801  5.092345\n",
            " 10.534221   2.7340856  9.741879   2.6943734  8.286178   6.9383397\n",
            "  8.362959   7.921925   1.5793267  1.9143265  7.550029   2.4284873\n",
            "  7.496504   5.329417   6.4065075  6.926101   5.869534   9.846267\n",
            "  8.652479   7.957929   6.3907094  9.189372   6.301699   6.9199867\n",
            "  4.50408    8.165175   7.787987   8.470692   2.2148867  3.4439611\n",
            "  7.638477   8.587035   6.824284   1.0634923  6.8592496  3.3233263\n",
            "  5.82946    7.183734   2.2960126  2.1245942  5.4235363  1.0924901\n",
            "  6.6250362  8.706917   7.1031175  6.5479417  4.2506022 10.09042\n",
            "  7.1962066  7.4835873  8.098946   6.404008  10.491569   1.7401701\n",
            "  6.0647817  6.557508   9.092578   6.6463556  6.50612    3.3512218\n",
            "  5.9616833  6.8869944  7.157903   8.577831   6.1310625  6.9435387\n",
            "  5.794928   7.878665   1.0748543  7.041807   8.094415   1.0689594\n",
            "  7.57213    6.7155533  7.4541764  3.222618   2.641341   7.1170716\n",
            "  8.9220705  7.093499   5.60567    3.5292408  4.71624    6.437217\n",
            "  6.0079556  5.430283   7.7271357  6.2119174  6.412698   7.194799\n",
            "  7.929335   4.29821    3.7876806 10.374624 ]\n",
            "Layer: backbone.extra.4.0, Channel Importance: [0.95784134 4.374378   4.403303   3.6786444  3.3230023  1.8055946\n",
            " 3.0753832  3.3974903  3.4751759  5.075972   4.507196   4.1208353\n",
            " 3.9871757  2.8349082  4.0237117  2.9494648  4.216873   3.8465576\n",
            " 3.3699553  3.0054607  1.4487269  3.765055   2.2894654  0.98759997\n",
            " 1.1859132  3.8775613  2.7407954  0.83999556 3.2726586  0.7451378\n",
            " 3.7063854  2.6850133  3.8372853  3.0568848  3.3493743  3.2068493\n",
            " 3.2937188  0.7139968  3.3849244  2.4626527  1.371378   2.9608476\n",
            " 2.2722516  3.8849468  4.590197   3.998648   3.2991312  3.509179\n",
            " 3.0665348  0.79807466 4.101874   4.609703   3.2290225  3.792832\n",
            " 1.5240782  3.8469913  4.8014116  3.5467176  2.153519   0.7686525\n",
            " 3.3052626  5.293633   2.4751184  3.9202595  3.1937933  4.6473975\n",
            " 3.4356344  2.72046    0.70287395 2.6773584  4.493661   3.7115958\n",
            " 1.4552972  4.77169    3.0451322  2.656587   2.9142475  4.0409527\n",
            " 4.398989   1.4436811  4.368712   3.4402833  4.691274   0.796358\n",
            " 2.3219285  4.18619    2.0064347  4.0268717  3.725047   1.6943055\n",
            " 0.82908344 0.7681345  2.047121   2.9438677  4.0956287  3.6361034\n",
            " 4.265529   4.244696   3.786812   3.7814183  1.4088715  3.8011203\n",
            " 4.2188344  4.580004   3.7912261  1.3603851  0.7340717  3.0760818\n",
            " 1.3816015  0.7040992  5.516131   2.7455835  2.9174488  4.555985\n",
            " 0.74777    2.4190032  3.6394587  0.95526785 3.6910274  1.6691067\n",
            " 3.7423186  4.2272196  3.612247   2.4586518  4.3052597  3.9448695\n",
            " 3.5429072  4.8462553 ]\n",
            "Layer: backbone.extra.4.2, Channel Importance: [ 4.580224   5.52688    1.1027633  5.362904   7.376306   3.4875166\n",
            "  6.0262914  6.7528195  1.2548068  5.810653   1.2966505  1.112596\n",
            "  6.1941476  6.0723467  5.4550734  4.946013   4.6207314  8.473594\n",
            "  6.0219617  1.0816706  5.124326   4.4312873  1.5400455  1.4199121\n",
            "  4.552089   6.8772793  2.6121738  1.0741315  7.895788   2.6264699\n",
            "  9.561103   4.639959   1.0852394  3.274193   6.2967114  1.0795976\n",
            "  5.8956885  1.0811164  5.3513694  7.9531336  8.625304   6.003985\n",
            "  1.071908   1.0608717  5.203187   5.642302   1.0733898  5.028488\n",
            "  4.6335964  6.1826563  7.558424   5.892421   9.619247   5.876523\n",
            "  7.561806   5.665305   6.1396317  3.6200173  6.5637026  1.0796682\n",
            "  6.9703135  4.9937162  6.2909317  1.0989957  6.943906   5.100698\n",
            "  6.5749373  5.888251   7.031308   7.0143204  6.8391356  5.5505495\n",
            "  1.0559629  6.8606944  1.0691006  4.3950696  6.1118536  5.234843\n",
            "  7.0978956  1.0811499  3.7798207  3.1606503  6.7246866  6.928235\n",
            "  7.453072   5.2556467  5.8270216  4.1193767  1.1883945  4.941905\n",
            "  9.876058   7.9936104  1.1354145  4.4531517  6.3532786  7.8067446\n",
            "  4.5459304  2.3496602  6.1333942  6.0077715  1.0745068  4.6229024\n",
            "  9.941375   1.0918907  6.8140235  7.8958154  6.82534    1.247171\n",
            "  1.0913163  7.3361754  6.355607   5.541961   7.923673   5.804456\n",
            "  1.0600454  1.1084099  8.215566   5.826034   6.3955703  3.571923\n",
            "  6.1985     7.039311   6.314075   5.6744313  7.3628893  8.273549\n",
            " 10.022316   1.0749966  5.3580685  5.584189   6.682213   6.095956\n",
            "  5.9050207  3.1902616  6.363661   6.058532   4.0808926  5.8857126\n",
            "  6.5433335  7.67114    7.482794   5.402267   5.619443   1.0787749\n",
            "  1.1013474  5.644752   5.7994604  1.8524948  3.4734974  5.509477\n",
            "  4.7448506  3.6426692  2.5620317  4.585228   8.043239   4.02817\n",
            "  8.098081   8.135024   6.000133   1.562933   6.0723004  1.3604203\n",
            "  1.093815   1.0977337  1.1002774  1.0759709  5.7000465  6.774778\n",
            "  1.2973267  6.7474666  5.2690144  2.2346222  1.0456493  5.1224174\n",
            "  4.616271   7.4002857  1.4850161  4.6224513  7.559186   7.7970853\n",
            "  6.4998946  5.6283636  6.6233664  1.0778913  2.424382   8.461226\n",
            "  5.5997114  5.6283307  7.0909233  8.6009245  3.6595755  2.4621239\n",
            "  4.98216    8.06887    4.5430374  7.667498   5.113976   5.005449\n",
            "  4.540924   5.990036   6.1839867  8.085018   5.786297   6.5187016\n",
            "  1.3438993  4.368437   6.823958   5.487477   3.620426   7.742632\n",
            "  5.362573   5.8128304  7.499307   4.5302105  6.60052    5.526843\n",
            "  6.041055   5.063973   1.625237   8.206094   1.5745372  5.6272125\n",
            "  8.136319   5.3959103  3.125276   4.601084   1.073414   2.4212868\n",
            "  8.000195   1.0672933  8.17817    6.5103784  5.9921803  6.6662226\n",
            "  1.4953058  5.5277605  7.193591   2.9433138  7.6202254  6.0128107\n",
            "  1.0771724  7.5788603  7.0357075  2.0980065  1.479911   6.5440083\n",
            "  1.1634738  5.3389378  6.534414   1.3205161  7.15918    2.7985673\n",
            "  1.045017   5.232112   6.7403445  7.2963905]\n",
            "Layer: head.classification_head.module_list.3, Channel Importance: [28.68357   18.85019    7.5702243  7.913823   8.6649275  8.352454\n",
            "  8.621522   8.4815645  8.31782    7.773272   7.0396004  7.5288305\n",
            "  2.6447973  7.582344   6.753025   8.55122    7.575988   9.858121\n",
            "  9.603548   8.335593   7.7874446  7.7904944  8.69675    7.948215\n",
            "  8.53142    9.053464   2.6263835  7.2822595  8.390196   2.6474004\n",
            "  2.6359777  7.164385   7.4831247  7.2077527  7.074577   7.546549\n",
            "  6.9563084  4.1147413  7.726025   6.805191   4.5410495  7.238295\n",
            "  7.251194   6.774321   6.774547   2.6198962  6.8939056  7.6238737\n",
            "  6.8566365  6.54632    6.5447326  8.535827   7.368604   6.9040136\n",
            "  8.551408   6.623406   7.9382854  6.543766   7.5843315  8.40337\n",
            "  7.8882413  7.7047057  9.762904   9.4241495  8.601727   9.088003\n",
            "  2.6464949 12.431256   2.639188   2.6144576  7.9874964  2.6426265\n",
            "  8.340696   8.859719   6.551846   6.636859   7.155886   7.2287483\n",
            "  7.062551   7.5139413  3.8796835  7.513868   8.518813   2.6439419\n",
            "  7.1900315  7.1256285  7.9493237  6.144055   8.310546   3.2367914\n",
            "  5.95971   31.843004  21.423231   7.8462424  8.376132   9.606752\n",
            "  9.505876   9.640509   9.614422   9.048758   8.421845   7.251045\n",
            "  8.189585   2.7136838  8.1598425  7.1330214  9.3649025  8.064628\n",
            " 10.99888   10.151562   9.053737   8.081989   8.478311   9.528918\n",
            "  8.495756   9.073165  10.186552   2.7066588  7.160229   9.272981\n",
            "  2.726931   2.692569   6.998569   7.826148   7.733045   7.0157094\n",
            "  7.620391   7.0328937  3.6246762  8.067905   7.028968   4.33539\n",
            "  7.2069273  7.4619217  6.73291    6.733436   2.7185082  7.3777027\n",
            "  7.6046643  7.3168006  6.7596164  6.970002   8.631782   7.4970675\n",
            "  7.118355   9.085163   6.9629965  8.349289   6.59856    8.187929\n",
            "  9.191135   8.097446   8.166654   9.881649  10.607514   8.711717\n",
            " 10.595925   2.718486  14.844466   2.72961    2.7053213  8.619422\n",
            "  2.7120261  8.752516   9.271663   6.651136   6.8938775  7.347421\n",
            "  7.6033163  7.533274   7.940926   4.3489857  7.705084   8.98606\n",
            "  2.7048922  7.290717   7.6102138  8.341002   6.46866    8.565291\n",
            "  3.5375872  6.166988  23.552992  14.263006   7.1406555  7.8477893\n",
            "  7.9260945  8.5413065  8.174263   8.547297   7.8776937  8.070415\n",
            "  6.0253882  5.8792357  2.5292952  6.6227384  5.4688053  8.48363\n",
            "  6.9289985  9.018794   8.332501   7.1839266  7.025887   7.146885\n",
            "  7.6379604  7.4035883  7.3492413  7.800229   2.5308666  6.090406\n",
            "  7.824979   2.5302496  2.4972548  5.3149943  6.185665   6.72217\n",
            "  6.363341   7.0617843  6.4171114  2.9592638  7.2686706  5.8451533\n",
            "  3.3727748  6.996932   7.374166   6.2915406  4.1567597  2.5435705\n",
            "  5.0184464  6.3826547  6.4830885  6.224194   6.0812626  7.434052\n",
            "  6.757546   6.316463   7.86636    5.971481   7.2857885  6.0005493\n",
            "  7.1802745  8.366306   7.341536   7.281      8.242142   9.099192\n",
            "  7.116643   9.104124   2.5337207 11.896259   2.5397003  2.5238864\n",
            "  6.8007097  2.518417   7.423992   7.751897   5.652471   6.180951\n",
            "  7.177633   6.381562   6.301989   6.917858   2.743221   7.51931\n",
            "  6.8823967  2.528736   6.7110333  6.378693   6.835265   5.655679\n",
            "  7.0241923  2.138469   5.3243575 25.547813  19.027092   6.865991\n",
            "  6.7961736  7.7455945  7.1028953  7.3940964  7.376587   7.3430505\n",
            "  7.01662    6.987315   7.596145   2.5073483  6.9936357  6.623183\n",
            "  7.383352   7.043875   9.0123825  8.71635    7.8156323  7.1040688\n",
            "  7.1169333  8.043694   7.4153366  7.9581537  9.007633   2.4772892\n",
            "  6.652208   7.0968575  2.5092351  2.5190523  6.4996037  7.6643558\n",
            "  6.5001645  5.8670163  7.093412   6.688909   2.9728506  7.192284\n",
            "  6.232308   3.0013638  6.316985   6.9959035  6.326122   7.7446113\n",
            "  2.494639   7.1880946  7.14453    6.0964527  5.5857735  5.4879165\n",
            "  7.457005   6.7712636  5.970428   7.724817   6.115873   7.073849\n",
            "  6.024265   6.914616   7.0431166  7.0906177  6.636256   8.593505\n",
            "  8.234561   8.402234   7.5754676  2.503202  10.226275   2.520716\n",
            "  2.5149634  7.3327665  2.4883184  7.4507446  7.656325   5.1549387\n",
            "  6.0951815  5.963758   7.0569205  6.106159   6.949403   3.1429248\n",
            "  6.88007    8.38211    2.500365   6.2172213  6.4592123  7.5005755\n",
            "  5.796184   7.8562317  2.9579542  6.117544  21.514198  11.460871\n",
            "  6.5175986  7.39057    6.8614564  8.3036375  7.461876   8.209352\n",
            "  7.331118   7.6302195  4.7038713  3.3703663  2.3985355  5.654132\n",
            "  2.9610655  8.045764   6.152284   7.923072   7.0941443  6.4209166\n",
            "  6.1417165  6.348195   6.7023916  6.6180234  6.4682283  6.582614\n",
            "  2.41542    4.9193373  7.327195   2.4165382  2.4394567  4.3805666\n",
            "  4.9825835  5.9374595  5.524046   6.677312   6.139237   1.9964072\n",
            "  6.528358   5.13495    2.436532   6.398614   7.1341105  5.568403\n",
            "  2.903793   2.4199328  2.9196386  5.1431885  5.9961014  5.722361\n",
            "  5.3508205  6.3555737  6.105173   5.4631124  6.87924    5.319127\n",
            "  6.381911   5.3682914  6.6090465  7.907449   6.397971   6.4948254\n",
            "  6.9327774  8.018495   5.932172   8.447597   2.4100392 11.281215\n",
            "  2.4306388  2.4047925  6.0193586  2.4246979  6.523404   6.817756\n",
            "  4.464021   5.2944126  6.889531   5.456802   5.6189523  6.167803\n",
            "  2.1573927  6.905762   5.619296   2.4153888  6.0967793  5.6929584\n",
            "  5.8748813  5.0762825  5.6831903  1.9751642  4.5767837 23.828945\n",
            " 18.520525   6.1375656  6.0852427  7.0655327  6.3492103  6.5077353\n",
            "  6.840383   6.435708   6.533402   6.3731303  7.3123326  2.4104865\n",
            "  6.327049   6.1045704  6.395501   6.571342   7.9726915  7.720214\n",
            "  7.5368032  6.677681   6.458182   7.4236245  6.7569785  7.1170034\n",
            "  8.579317   2.4044788  5.5548296  5.8991995  2.4386299  2.421852\n",
            "  5.6345873  7.365999   5.883668   3.7590153  6.6702223  6.387433\n",
            "  1.999164   6.4883614  5.3624487  2.0126178  5.755669   6.7930274\n",
            "  5.899776   7.367414   2.4028692  6.680828   6.4260926  5.4102755\n",
            "  4.930388   4.544218   6.129217   6.1260138  5.3575506  6.5464396\n",
            "  5.553556   6.3094273  5.193112   6.053451   5.797185   5.873185\n",
            "  5.454208   7.598483   7.010671   7.7275696  6.4578776  2.391004\n",
            "  7.748913   2.4104862  2.3965013  6.7490406  2.4074028  6.314484\n",
            "  6.3471966  3.837027   5.6712155  4.987808   6.8716974  4.4834433\n",
            "  6.475952   2.3613243  5.9952154  7.9657955  2.420201   5.5840197\n",
            "  5.9068007  7.1194563  5.2844205  7.0654416  2.2862034  5.8265276]\n"
          ]
        }
      ],
      "source": [
        "small_importance_scores = compute_channel_importance(model, small_dataloader, beta, T=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-WZj84dzCz3"
      },
      "source": [
        "**A layer in your model** → E.g., backbone.features.0\n",
        "\n",
        "**Importance scores for each channel in that layer** → E.g., [6.955693, 3.0487685, 2.3222566, ...]\n",
        "\n",
        "**Higher importance values** → Channels that contribute more to feature extraction.\n",
        "\n",
        "**Lower importance values** → Channels that may be less significant and could be pruned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvK-IKcd0_PV",
        "outputId": "60a55576-c16f-429f-af06-01294d42149a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer: backbone.features.0, Min Importance: 1.5060, Max Importance: 8.0641\n",
            "Layer: backbone.features.2, Min Importance: 6.9783, Max Importance: 23.0242\n",
            "Layer: backbone.features.5, Min Importance: 4.4219, Max Importance: 23.8352\n",
            "Layer: backbone.features.7, Min Importance: 13.9452, Max Importance: 150.5465\n",
            "Layer: backbone.features.10, Min Importance: 12.1955, Max Importance: 483.0923\n",
            "Layer: backbone.features.12, Min Importance: 27.2105, Max Importance: 1648.9426\n",
            "Layer: backbone.features.14, Min Importance: 41.8643, Max Importance: 8731.9932\n",
            "Layer: backbone.features.17, Min Importance: 45.4501, Max Importance: 11761.2686\n",
            "Layer: backbone.features.19, Min Importance: 155.2118, Max Importance: 29862.6445\n",
            "Layer: backbone.features.21, Min Importance: 513.5262, Max Importance: 101972.2656\n",
            "Layer: backbone.extra.0.1, Min Importance: 3280.5598, Max Importance: 349468.0312\n",
            "Layer: backbone.extra.0.3, Min Importance: 13316.2988, Max Importance: 3390896.7500\n",
            "Layer: backbone.extra.0.5, Min Importance: 12225.2539, Max Importance: 24857296.0000\n",
            "Layer: backbone.extra.0.7.1, Min Importance: 6611.5332, Max Importance: 40088036.0000\n",
            "Layer: backbone.extra.0.7.3, Min Importance: 5281.2852, Max Importance: 11685247.0000\n",
            "Layer: backbone.extra.1.0, Min Importance: 6214.6182, Max Importance: 45430368.0000\n",
            "Layer: backbone.extra.1.2, Min Importance: 1102.6498, Max Importance: 136304208.0000\n",
            "Layer: backbone.extra.2.0, Min Importance: 9214.6104, Max Importance: 457244992.0000\n",
            "Layer: backbone.extra.2.2, Min Importance: 12333.9180, Max Importance: 1396352768.0000\n",
            "Layer: backbone.extra.3.0, Min Importance: 436289.0000, Max Importance: 2010998400.0000\n",
            "Layer: backbone.extra.3.2, Min Importance: 42213.8242, Max Importance: 7830412800.0000\n",
            "Layer: backbone.extra.4.0, Min Importance: 256494.2188, Max Importance: 10811438080.0000\n",
            "Layer: backbone.extra.4.2, Min Importance: 176089.8438, Max Importance: 61331099648.0000\n",
            "Layer: head.classification_head.module_list.3, Min Importance: 1579358.8750, Max Importance: 23559653376.0000\n"
          ]
        }
      ],
      "source": [
        "for layer, scores in importance_scores.items():\n",
        "    print(f\"Layer: {layer}, Min Importance: {np.min(scores):.4f}, Max Importance: {np.max(scores):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9SZHPJQ8Gpc"
      },
      "source": [
        "It can be assumed:\n",
        "*   Pruning should be more aggressive in features.0, features.2, and features.5\n",
        "*   Less pruning in backbone.features.12,14,17,19,21 extract strong object features.\n",
        "*  Backbone.extra: Minimal pruning here. Detect objects at multiple scales. Very high max values, layers are critical.\n",
        "*   No pruning in head.classification_head.module_list.3, as it directly affects classification accuracy.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-ivnePuA1kc"
      },
      "source": [
        "# **Structured table (DataFrame) of channel importance scores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSmQDKgEAio1",
        "outputId": "de93049c-9ff0-4af3-d3b1-3258370d09ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========================================\n",
            "🔹 LOWEST 5 IMPORTANCE SCORES 🔹\n",
            "========================================\n",
            "              Layer  Channel Index  Importance Score\n",
            "backbone.features.0             24          1.506048\n",
            "backbone.features.0             32          1.701681\n",
            "backbone.features.0             40          2.263218\n",
            "backbone.features.0              2          2.324010\n",
            "backbone.features.0             52          2.468707\n",
            "\n",
            "========================================\n",
            "🔹 HIGHEST 5 IMPORTANCE SCORES 🔹\n",
            "========================================\n",
            "             Layer  Channel Index  Importance Score\n",
            "backbone.extra.4.2             82      4.203724e+10\n",
            "backbone.extra.4.2            199      4.247995e+10\n",
            "backbone.extra.4.2            118      4.933298e+10\n",
            "backbone.extra.4.2             52      5.167216e+10\n",
            "backbone.extra.4.2             66      6.133110e+10\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Check if importance_scores exists\n",
        "if 'importance_scores' in globals() and isinstance(importance_scores, dict) and len(importance_scores) > 0:\n",
        "    # Flatten scores and store with corresponding layers\n",
        "    data = []\n",
        "    for layer, scores in importance_scores.items():\n",
        "        for i, score in enumerate(scores):\n",
        "            data.append([layer, i, score])  # Store (Layer Name, Channel Index, Importance Score)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data, columns=[\"Layer\", \"Channel Index\", \"Importance Score\"])\n",
        "\n",
        "    # Sort by Importance Score\n",
        "    df_sorted = df.sort_values(by=\"Importance Score\", ascending=True)\n",
        "\n",
        "    # Print lowest 5 importance scores\n",
        "    print(\"=\" * 40)\n",
        "    print(\"🔹 LOWEST 5 IMPORTANCE SCORES 🔹\")\n",
        "    print(\"=\" * 40)\n",
        "    print(df_sorted.head(5).to_string(index=False))\n",
        "\n",
        "    # Print highest 5 importance scores\n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    print(\"🔹 HIGHEST 5 IMPORTANCE SCORES 🔹\")\n",
        "    print(\"=\" * 40)\n",
        "    print(df_sorted.tail(5).to_string(index=False))\n",
        "\n",
        "else:\n",
        "    print(\"Error: `importance_scores` is not defined or empty. Please run Algorithm 1 first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9RXhyMHCVSz",
        "outputId": "e1511344-b8d6-4e77-e4fa-6c1694e22f44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer\n",
            "backbone.features.0    16\n",
            "Name: Channel Index, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Define a threshold for low-importance channels (adjust if needed)\n",
        "threshold = 3.0  #Importance Score < 3.0 is considered low importance\n",
        "\n",
        "# Count low-importance channels per layer\n",
        "layer_counts = df_sorted[df_sorted[\"Importance Score\"] < threshold].groupby(\"Layer\")[\"Channel Index\"].count() #Keeps only channels that have an importance score lower than 3.0\n",
        "\n",
        "# Display results\n",
        "print(layer_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULN-I7ItEB7p"
      },
      "source": [
        "*   Only backbone.features.0 has low-importance channels (with Importance Score < 3.0)\n",
        "\n",
        "*   A total of 16 channels in this layer are weak and could be potential pruning candidates\n",
        "\n",
        "*   No other layers have low-importance channels under the current threshold (3.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uxgOqpOE5UK",
        "outputId": "6a830058-1ac2-4d93-81f1-4585c6917ffc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer\n",
            "backbone.features.0    43\n",
            "backbone.features.5     1\n",
            "Name: Channel Index, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "threshold = 5.0  # Increase threshold to find more low-importance channels\n",
        "layer_counts = df_sorted[df_sorted[\"Importance Score\"] < threshold].groupby(\"Layer\")[\"Channel Index\"].count()\n",
        "print(layer_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWWMNyEWE9Z_"
      },
      "source": [
        "*   Only backbone.features.0 and backbone.features.5  has low-importance channels (with Importance Score < 5.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvv9r5FbFKbQ"
      },
      "source": [
        "# **Algorithm 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oljFqJq9Gp2s"
      },
      "source": [
        "*   Grouping channels based on importance scores\n",
        "*   Computing the cost of keeping or pruning each group (FLOPs & Params)\n",
        "*   Preparing importance values for the knapsack solver (Algorithm 3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "auQEnjgSPWKf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def knapsack_initialize(model, channel_importance, beta, k=0.5, b=0.1, num_groups=3):\n",
        "    \"\"\"\n",
        "    Initializes the knapsack problem by grouping channels and calculating values/constraints.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The pre-trained model.\n",
        "    - channel_importance: Dictionary of channel importance scores per layer.\n",
        "    - beta: Scenario complexity value (0 to 1).\n",
        "    - k: Controls sparsity (higher -> preserves more channels).\n",
        "    - b: Minimum fraction of channels to keep.\n",
        "    - num_groups: Number of groups to aggregate channels.\n",
        "\n",
        "    Returns:\n",
        "    - values (v): Importance values for each group.\n",
        "    - weights_F (wF): FLOPs cost per group.\n",
        "    - weights_P (wP): Parameters cost per group.\n",
        "    \"\"\"\n",
        "\n",
        "    values, weights_F, weights_P = [], [], []     #weights_F → Stores FLOPs cost per group. weights_P → Stores parameter count per group.\n",
        "    total_items = 0  # Initialize t\n",
        "\n",
        "    for layer_name, importance_scores in channel_importance.items():\n",
        "        num_channels = len(importance_scores)\n",
        "\n",
        "        # Compute lower bound of channels to keep\n",
        "        eta = k * (1 - (1 - beta) * np.sqrt(len(values) / len(channel_importance))) + b  #determine how many channels must be preserved\n",
        "        num_preserved = min(int(num_channels * eta), num_channels)  # Ensure valid range\n",
        "\n",
        "        # Sort channels by importance (descending order)\n",
        "        sorted_indices = np.argsort(-importance_scores)  #Sorted from most important to least important\n",
        "\n",
        "        # Select channels to prune & preserve\n",
        "        pruned_channels = sorted_indices[num_preserved:]   #Keeps the top num_preserved channels and marks the rest for pruning\n",
        "\n",
        "        # Group channels for efficient pruning\n",
        "        grouped_pruned_channels = np.array_split(pruned_channels, num_groups)\n",
        "\n",
        "        # Get layer from model\n",
        "        layer = dict(model.named_modules()).get(layer_name, None)\n",
        "        if layer is None or not hasattr(layer, \"weight\"):\n",
        "            continue  # Skip if layer is not found or has no weights\n",
        "\n",
        "        for group in grouped_pruned_channels:\n",
        "            if len(group) == 0:\n",
        "                continue  # Skip empty groups\n",
        "\n",
        "            # Compute importance, FLOPs, and Params for the group\n",
        "            group_importance = sum(importance_scores[i] for i in group)\n",
        "            group_FLOPs = sum(layer.weight.view(-1)[i].numel() for i in group)  # Corrected access\n",
        "            group_Params = sum(layer.weight.view(-1)[i].numel() for i in group)  # Corrected access\n",
        "\n",
        "            values.append(group_importance)\n",
        "            weights_F.append(group_FLOPs)\n",
        "            weights_P.append(group_Params)\n",
        "\n",
        "            total_items += 1  # Increment t\n",
        "\n",
        "    return values, weights_F, weights_P  # Return after processing all layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBRjOGltPXer",
        "outputId": "a07d7b4b-094e-448c-9e84-10341671e0ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Values (Importance Scores per Group of channels): [28.469270944595337, 25.969562292099, 17.970082640647888, 137.91753101348877, 106.73486804962158, 79.29806184768677, 246.0164556503296, 200.88183879852295, 137.8762845993042, 475.997013092041, 409.4109344482422, 329.2639980316162, 795.8137302398682, 716.9345188140869, 634.49573802948, 1875.8890266418457, 1573.1683464050293, 1284.4642658233643, 5917.722557067871, 3915.806312561035, 2592.196704864502, 35572.9274597168, 23729.98471069336, 13050.783966064453, 126106.24743652344, 84675.94262695312, 46889.04325866699, 595721.2578125, 374117.2048339844, 175674.7889404297, 2915488.931640625, 1973982.849609375, 828952.2683105469, 16861097.609375, 10152533.8046875, 4231944.27734375, 81014754.625, 39934734.78125, 13484132.23828125, 127090898.8125, 71407178.78125, 26754324.297851562, 73486166.9375, 38336088.734375, 10673655.149902344, 60827949.0, 34514347.0, 11135156.3984375, 528969168.0, 265049778.5, 82380074.76623535, 391805340.0, 189590952.5, 35642337.54785156, 2641321584.0, 1434208536.0, 400030748.4746094, 1667434616.0, 866193794.0, 173292101.875, 11664644576.0, 4542085136.0, 791216431.9414062, 8685591328.0, 2801290968.0, 317057390.96875, 44944534848.0, 9796027128.0, 272168999.75, 18289677072.0, 6300554376.0, 1113744729.375]\n",
            "Weights (FLOPs per Group): [9, 9, 8, 10, 9, 9, 19, 19, 18, 19, 19, 19, 39, 38, 38, 39, 39, 38, 40, 39, 39, 79, 79, 79, 80, 80, 80, 81, 81, 80, 82, 81, 81, 82, 82, 81, 83, 82, 82, 166, 166, 165, 167, 167, 167, 42, 42, 42, 85, 85, 84, 22, 21, 21, 43, 43, 43, 22, 22, 21, 44, 43, 43, 22, 22, 22, 44, 44, 43, 94, 94, 93]\n",
            "Weights (Params per Group): [9, 9, 8, 10, 9, 9, 19, 19, 18, 19, 19, 19, 39, 38, 38, 39, 39, 38, 40, 39, 39, 79, 79, 79, 80, 80, 80, 81, 81, 80, 82, 81, 81, 82, 82, 81, 83, 82, 82, 166, 166, 165, 167, 167, 167, 42, 42, 42, 85, 85, 84, 22, 21, 21, 43, 43, 43, 22, 22, 21, 44, 43, 43, 22, 22, 22, 44, 44, 43, 94, 94, 93]\n"
          ]
        }
      ],
      "source": [
        "values, weights_F, weights_P = knapsack_initialize(model, importance_scores, beta)\n",
        "\n",
        "print(\"Values (Importance Scores per Group of channels):\", values)\n",
        "print(\"Weights (FLOPs per Group):\", weights_F)\n",
        "print(\"Weights (Params per Group):\", weights_P)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVgtHicaIvQV"
      },
      "source": [
        "*   The smallest importance score is 9.91, while the largest exceeds 167.67M\n",
        "\n",
        "*   FLOPs(determine computational cost) and Parameter costs are identical (common in CNNs), low FLOPs (2-3) → Less computational cost.\n",
        "\n",
        "\n",
        "*   High FLOPs contribute significantly to model performance (33,34,15 etc)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKbAAAjgIl-5"
      },
      "source": [
        "# **Algorithm 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i45uUW4KTAw"
      },
      "source": [
        " *  Selects the best channel groups to keep based on their importance, FLOPs, and parameter constraints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oU8jm2lr_28o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def knapsack_solver(values, weights_F, weights_P, R_F, R_P):\n",
        "    \"\"\"\n",
        "    Solves the multi-constraint knapsack problem to select optimal groups of channels.\n",
        "\n",
        "    Parameters:\n",
        "    - values (v): Importance values for each group.\n",
        "    - weights_F (wF): FLOPs cost per group.\n",
        "    - weights_P (wP): Parameters cost per group.\n",
        "    - R_F: FLOPs pruning constraint (fraction of FLOPs to keep).\n",
        "    - R_P: Parameter pruning constraint (fraction of Params to keep).\n",
        "\n",
        "    Returns:\n",
        "    - selected_indices: Indices of the selected channel groups.\n",
        "    \"\"\"\n",
        "\n",
        "    # Number of items (groups)\n",
        "    t = len(values)\n",
        "\n",
        "    # Compute FLOPs and Parameter constraints, Computes how many FLOPs & Params can be removed, based on initial ratios we provide\n",
        "    C_F = int((1 - R_F) * sum(weights_F))  # Target FLOPs limit\n",
        "    C_P = int((1 - R_P) * sum(weights_P))  # Target Parameter limit\n",
        "\n",
        "    # Initialize DP table, Dynamic Programming (DP) to find the optimal selection\n",
        "    A = np.zeros((t + 1, C_F + 1, C_P + 1))\n",
        "\n",
        "    # Fill DP table (Knapsack Logic)\n",
        "    for i in range(1, t + 1): #Iterates over all groups (t), FLOPs (C_F), and Params (C_P)\n",
        "        for c_F in range(C_F + 1):\n",
        "            for c_P in range(C_P + 1):\n",
        "                if weights_F[i-1] > c_F or weights_P[i-1] > c_P:\n",
        "                    A[i, c_F, c_P] = A[i-1, c_F, c_P]  # Cannot include this group, if a group exceeds the FLOP or Param limit, it is skipped\n",
        "                else:\n",
        "                    A[i, c_F, c_P] = max(A[i-1, c_F, c_P],\n",
        "                                         values[i-1] + A[i-1, c_F - weights_F[i-1], c_P - weights_P[i-1]])\n",
        "\n",
        "    # Backtrack to find selected groups\n",
        "    selected_indices = []\n",
        "    i, c_F, c_P = t, C_F, C_P\n",
        "\n",
        "    while i > 0 and c_F > 0 and c_P > 0:\n",
        "        if A[i, c_F, c_P] != A[i-1, c_F, c_P]:\n",
        "            selected_indices.append(i-1)\n",
        "            c_F -= weights_F[i-1]\n",
        "            c_P -= weights_P[i-1]\n",
        "        i -= 1\n",
        "\n",
        "    return selected_indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wgxgpYn7FjE",
        "outputId": "64ae5ece-14c1-4820-89ea-6c7b5efc7a1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Selected channels (Indices): [71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 19, 18, 3]\n",
            "\\Selected Groups Details:\n",
            "channel 71: Importance = 1113744729.375, FLOPs = 93, Params = 93\n",
            "channel 70: Importance = 6300554376.0, FLOPs = 94, Params = 94\n",
            "channel 69: Importance = 18289677072.0, FLOPs = 94, Params = 94\n",
            "channel 68: Importance = 272168999.75, FLOPs = 43, Params = 43\n",
            "channel 67: Importance = 9796027128.0, FLOPs = 44, Params = 44\n",
            "channel 66: Importance = 44944534848.0, FLOPs = 44, Params = 44\n",
            "channel 65: Importance = 317057390.96875, FLOPs = 22, Params = 22\n",
            "channel 64: Importance = 2801290968.0, FLOPs = 22, Params = 22\n",
            "channel 63: Importance = 8685591328.0, FLOPs = 22, Params = 22\n",
            "channel 62: Importance = 791216431.9414062, FLOPs = 43, Params = 43\n",
            "channel 61: Importance = 4542085136.0, FLOPs = 43, Params = 43\n",
            "channel 60: Importance = 11664644576.0, FLOPs = 44, Params = 44\n",
            "channel 59: Importance = 173292101.875, FLOPs = 21, Params = 21\n",
            "channel 58: Importance = 866193794.0, FLOPs = 22, Params = 22\n",
            "channel 57: Importance = 1667434616.0, FLOPs = 22, Params = 22\n",
            "channel 56: Importance = 400030748.4746094, FLOPs = 43, Params = 43\n",
            "channel 55: Importance = 1434208536.0, FLOPs = 43, Params = 43\n",
            "channel 54: Importance = 2641321584.0, FLOPs = 43, Params = 43\n",
            "channel 53: Importance = 35642337.54785156, FLOPs = 21, Params = 21\n",
            "channel 52: Importance = 189590952.5, FLOPs = 21, Params = 21\n",
            "channel 51: Importance = 391805340.0, FLOPs = 22, Params = 22\n",
            "channel 50: Importance = 82380074.76623535, FLOPs = 84, Params = 84\n",
            "channel 49: Importance = 265049778.5, FLOPs = 85, Params = 85\n",
            "channel 48: Importance = 528969168.0, FLOPs = 85, Params = 85\n",
            "channel 47: Importance = 11135156.3984375, FLOPs = 42, Params = 42\n",
            "channel 46: Importance = 34514347.0, FLOPs = 42, Params = 42\n",
            "channel 45: Importance = 60827949.0, FLOPs = 42, Params = 42\n",
            "channel 44: Importance = 10673655.149902344, FLOPs = 167, Params = 167\n",
            "channel 43: Importance = 38336088.734375, FLOPs = 167, Params = 167\n",
            "channel 42: Importance = 73486166.9375, FLOPs = 167, Params = 167\n",
            "channel 41: Importance = 26754324.297851562, FLOPs = 165, Params = 165\n",
            "channel 40: Importance = 71407178.78125, FLOPs = 166, Params = 166\n",
            "channel 39: Importance = 127090898.8125, FLOPs = 166, Params = 166\n",
            "channel 38: Importance = 13484132.23828125, FLOPs = 82, Params = 82\n",
            "channel 37: Importance = 39934734.78125, FLOPs = 82, Params = 82\n",
            "channel 36: Importance = 81014754.625, FLOPs = 83, Params = 83\n",
            "channel 35: Importance = 4231944.27734375, FLOPs = 81, Params = 81\n",
            "channel 34: Importance = 10152533.8046875, FLOPs = 82, Params = 82\n",
            "channel 33: Importance = 16861097.609375, FLOPs = 82, Params = 82\n",
            "channel 32: Importance = 828952.2683105469, FLOPs = 81, Params = 81\n",
            "channel 31: Importance = 1973982.849609375, FLOPs = 81, Params = 81\n",
            "channel 30: Importance = 2915488.931640625, FLOPs = 82, Params = 82\n",
            "channel 29: Importance = 175674.7889404297, FLOPs = 80, Params = 80\n",
            "channel 28: Importance = 374117.2048339844, FLOPs = 81, Params = 81\n",
            "channel 27: Importance = 595721.2578125, FLOPs = 81, Params = 81\n",
            "channel 26: Importance = 46889.04325866699, FLOPs = 80, Params = 80\n",
            "channel 25: Importance = 84675.94262695312, FLOPs = 80, Params = 80\n",
            "channel 24: Importance = 126106.24743652344, FLOPs = 80, Params = 80\n",
            "channel 23: Importance = 13050.783966064453, FLOPs = 79, Params = 79\n",
            "channel 22: Importance = 23729.98471069336, FLOPs = 79, Params = 79\n",
            "channel 21: Importance = 35572.9274597168, FLOPs = 79, Params = 79\n",
            "channel 19: Importance = 3915.806312561035, FLOPs = 39, Params = 39\n",
            "channel 18: Importance = 5917.722557067871, FLOPs = 40, Params = 40\n",
            "channel 3: Importance = 137.91753101348877, FLOPs = 10, Params = 10\n"
          ]
        }
      ],
      "source": [
        "# Define pruning constraints (adjust as needed)\n",
        "R_F = 0.1  # Keep 30% of FLOPs\n",
        "R_P = 0.1  # Keep 30% of Params\n",
        "\n",
        "# Run the knapsack solver with previously computed values\n",
        "selected_channels = knapsack_solver(values, weights_F, weights_P, R_F, R_P)\n",
        "\n",
        "# Print selected groups\n",
        "print(\" Selected channels (Indices):\", selected_channels)\n",
        "\n",
        "# Print selected groups with their importance, FLOPs, and Params\n",
        "print(\"\\Selected Groups Details:\")\n",
        "for idx in selected_channels:\n",
        "    print(f\"channel {idx}: Importance = {values[idx]}, FLOPs = {weights_F[idx]}, Params = {weights_P[idx]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWSr5qSc6Oj9"
      },
      "source": [
        "# **Pruning the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KVMF8aOim5On"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "def prune_ssd_vgg16(original_model, selected_channels_per_layer):\n",
        "    \"\"\"\n",
        "    Prunes SSD-VGG16 by removing unselected channels and ensuring input-output consistency.\n",
        "    \"\"\"\n",
        "    pruned_backbone = nn.Sequential()\n",
        "    prev_out_channels = None\n",
        "\n",
        "    for name, layer in original_model.backbone.named_children():\n",
        "        if isinstance(layer, nn.Conv2d) and name in selected_channels_per_layer:\n",
        "            selected = selected_channels_per_layer[name]\n",
        "\n",
        "            if len(selected) == 0:\n",
        "                print(f\" Warning: No channels left in {name}. Keeping at least 1.\")\n",
        "                selected = [0]  # Keep at least 1 channel\n",
        "\n",
        "            # Prune Weights & Bias\n",
        "            new_weights = layer.weight[selected, :, :, :].clone().detach()\n",
        "            new_bias = layer.bias[selected].clone().detach() if layer.bias is not None else None\n",
        "\n",
        "            # Set Input Channels from Previous Layer\n",
        "            in_channels = prev_out_channels if prev_out_channels is not None else layer.in_channels\n",
        "            selected_in_channels = list(range(in_channels))  # Keep all input channels\n",
        "\n",
        "            # Create Pruned Conv2D Layer\n",
        "            pruned_layer = nn.Conv2d(\n",
        "                in_channels=len(selected_in_channels),\n",
        "                out_channels=len(selected),\n",
        "                kernel_size=layer.kernel_size,\n",
        "                stride=layer.stride,\n",
        "                padding=layer.padding,\n",
        "                bias=(layer.bias is not None)\n",
        "            )\n",
        "\n",
        "            # Assign Pruned Weights & Bias\n",
        "            pruned_layer.weight = nn.Parameter(new_weights)\n",
        "            if new_bias is not None:\n",
        "                pruned_layer.bias = nn.Parameter(new_bias)\n",
        "\n",
        "            # Add to Pruned Backbone\n",
        "            pruned_backbone.add_module(name, pruned_layer)\n",
        "            prev_out_channels = len(selected)  # Update Output Channels\n",
        "\n",
        "            print(f\"Pruned Layer: {name}, Before: {layer.out_channels}, After: {len(selected)}\")\n",
        "\n",
        "        else:\n",
        "            pruned_backbone.add_module(name, layer)  # Keep Non-Conv Layers\n",
        "\n",
        "    # Create New Pruned SSD Model\n",
        "    pruned_model = models.detection.ssd300_vgg16(pretrained=False)\n",
        "    pruned_model.backbone = pruned_backbone\n",
        "\n",
        "    return pruned_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWCGwLMDpfw2",
        "outputId": "50506012-6e15-4f92-c5e4-953e3c8c0930"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16_features-amdegroot-88682ab5.pth\" to /root/.cache/torch/hub/checkpoints/vgg16_features-amdegroot-88682ab5.pth\n",
            "100%|██████████| 528M/528M [00:07<00:00, 75.3MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pruned model created successfully!\n"
          ]
        }
      ],
      "source": [
        "pruned_model = prune_ssd_vgg16(model, selected_channels)\n",
        "print(\"Pruned model created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knuwJrYFHJh8",
        "outputId": "1a0476fa-87a8-4de7-a8fd-a4943df29e12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Model Params: 35641826\n",
            "Pruned Model Params: 35641314\n"
          ]
        }
      ],
      "source": [
        "original_params = sum(p.numel() for p in model.parameters())\n",
        "pruned_params = sum(p.numel() for p in pruned_model.parameters())\n",
        "print(f\"Original Model Params: {original_params}\")\n",
        "print(f\"Pruned Model Params: {pruned_params}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DULHcgGZi79G"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_model\u001b[39m(model, dataloader):\n\u001b[32m      4\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m    Evaluates the SSD model by checking if the highest confidence prediction matches the ground truth.\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    \"\"\"\n",
        "    Evaluates the SSD model by checking if the highest confidence prediction matches the ground truth.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            outputs = model(images)\n",
        "\n",
        "            for i in range(len(outputs)):\n",
        "                scores = outputs[i]['scores']  # Get confidence scores\n",
        "                if len(scores) > 0:  # Ensure at least one detection exists\n",
        "                    highest_score_idx = torch.argmax(scores).item()\n",
        "                    predicted_label = outputs[i]['labels'][highest_score_idx].item()\n",
        "                    if predicted_label == labels[i].item():  # Match with true label\n",
        "                        correct += 1\n",
        "                total += 1\n",
        "\n",
        "    return correct / total if total > 0 else 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "idhbzq4EDgEJ",
        "outputId": "fdf71129-204f-46ed-f450-a3620a8b3155"
      },
      "outputs": [
        {
          "ename": "NotImplementedError",
          "evalue": "Module [ModuleList] is missing the required \"forward\" function",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-3a30a3be0dac>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpruned_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpruned_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" Pruned Model Accuracy: {pruned_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-03d5784f4b58>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/detection/ssd.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;31m# get the features from the backbone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \"\"\"\n\u001b[0;32m--> 394\u001b[0;31m     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;34mf'Module [{type(self).__name__}] is missing the required \"forward\" function'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Module [ModuleList] is missing the required \"forward\" function"
          ]
        }
      ],
      "source": [
        "pruned_acc = evaluate_model(pruned_model, test_dataloader)\n",
        "print(f\" Pruned Model Accuracy: {pruned_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hacvj5HvEKm2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
