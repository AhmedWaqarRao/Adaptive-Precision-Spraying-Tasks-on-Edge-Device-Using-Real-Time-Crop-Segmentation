import os
import cv2
import numpy as np
import requests
import torch
import torchvision.models as models
from torchvision import datasets, transforms
import matplotlib.pyplot as plt

# Define a function to download a simple dataset (CIFAR-10)
def download_cifar10(data_path):
    transform = transforms.Compose([
        transforms.ToTensor()  # Keep RGB channels (3 channels instead of Grayscale)
    ])
    dataset = datasets.CIFAR10(root=data_path, train=True, download=True, transform=transform)
    return dataset

# Compute Scenario Complexity using Pixel Entropy
def compute_scenario_complexity(dataset):
    entropy_values = []
    
    for img, _ in dataset:  # Iterate through images
        img = torch.mean(img, dim=0).numpy() * 255  # Convert tensor to numpy and grayscale
        hist = cv2.calcHist([img.astype(np.uint8)], [0], None, [256], [0, 256])
        hist = hist / np.sum(hist)  # Normalize histogram
        entropy = -np.sum(hist * np.log2(hist + 1e-10))  # Compute entropy
        entropy_values.append(entropy)
    
    if not entropy_values:
        raise ValueError("No valid entropy values computed. Check dataset frames.")
    
    beta = np.mean(entropy_values) / np.log2(256)  # Normalize to [0,1]
    return beta, entropy_values

# Compute Channel Importance
def compute_channel_importance(model, dataloader, beta, T=1):
    importance_scores = {}
    model.eval()
    
    with torch.no_grad():
        for batch in dataloader:
            X = batch[0].to(next(model.parameters()).device)  # Move to the same device as model
            
            if X.shape[1] == 1:  # If grayscale, convert to 3 channels
                X = X.repeat(1, 3, 1, 1)
            
            for name, layer in model.named_modules():
                if isinstance(layer, torch.nn.Conv2d):
                    try:
                        if X.shape[1] != layer.weight.shape[1]:  # Adjust input channels if mismatched
                            continue
                        X = layer(X)  # Pass input through layer
                        
                        # Dynamic Importance (Variance of activations)
                        mean_per_channel = torch.mean(X, dim=0, keepdim=True)
                        dynamic_importance = torch.mean((X - mean_per_channel) ** 2, dim=[0, 2, 3])
                        
                        # Static Importance (L1 norm of weights)
                        static_importance = torch.norm(layer.weight, p=1, dim=[1, 2, 3])
                        
                        # Compute combined channel importance
                        alpha = (beta ** 0.5) / T
                        channel_importance = (1 - alpha) * dynamic_importance + alpha * static_importance
                        
                        importance_scores[name] = channel_importance.cpu().numpy()
                        print(f"Layer: {name}, Channel Importance: {channel_importance.cpu().numpy()}")  # Print channel importance per layer
                    except Exception as e:
                        print(f"Error processing layer {name}: {e}")
            break  # Process only one batch for efficiency
    
    return importance_scores

# Download dataset
data_path = "./cifar10_data"
dataset = download_cifar10(data_path)
dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False)

# Compute scenario complexity
beta, entropy_values = compute_scenario_complexity(dataset)
print(f"Scenario Complexity (Beta): {beta}")

# Load pre-trained SSD-VGG model
model = models.detection.ssd300_vgg16(pretrained=True)

# Compute channel importance
importance_scores = compute_channel_importance(model, dataloader, beta, T=1)
print("Channel Importance Scores Computed")

# Visualize entropy distribution
plt.hist(entropy_values, bins=30, color='blue', alpha=0.7)
plt.xlabel("Entropy")
plt.ylabel("Frequency")
plt.title("Entropy Distribution in CIFAR-10 Dataset")
plt.show()
